{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder - Classificação com autoencoder vs sem autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo Autoencoder - Algoritmo não supervisionado. \n",
    "# É um algoritmo self-supervised learning, aprendizagem por si próprio. O que o algoritmo faz é aprender a classificar os\n",
    "# registros da camada de saida de acordo com os valores da camada de entrada. O algoritmo aprende a codificaar e descodificar\n",
    "# por si próprio. É como se os registros codificados fossem uma classe.\n",
    "\n",
    "# Tem uma camada de entrada, uma camada oculta e a camada de saida.\n",
    "# O nº de neurónios da camada de saida é igual ao da camada de entrada.\n",
    "# Cada neuronio da camada de entrada está ligado a todos os neurónios da camada oculta(fully connected).\n",
    "# Cada neurónio da camada oculta está ligado a todos os neurónios da camada de saida(fully connected).\n",
    "\n",
    "# Da camada de entrada para a camada oculta faz-se uma codificação dos valores da camada de entrada utilizando os pesos.\n",
    "# Por ex. tem-se na camada de entrada (1 0 1 0 1) faz-se uma codificação e tem-se na camada oculta esses valores codificados\n",
    "# por exemplo (2 3). Da camada oculta para a camada de saida faz-se a descodificação, utilizando os pesos, e tem-se na camada \n",
    "# de saida o valor original da camada de entrada (1 0 1 0 1).\n",
    "\n",
    "# O processo é parecido às redes neurais classicas feed-forward. Utiliza-se tambem o back-propagation para a actualização dos \n",
    "# pesos.\n",
    "# São atribuidos pesos aos neuronios da rede e os calculos são efectuados com esses pesos partindo da camada de entrada até a \n",
    "# camada de saida. \n",
    "# Calcula-se o erro obtido na camada de saida e modificam-se os pesos da iteração anterior partindo da camada de saida até a \n",
    "# camada de entrada. Utiliza-se o método da descida do gradiente para encontrar o melhor valor para os pesos.\n",
    "# De seguida, e com os pesos obtidos na iteração anterior, são efectuados os mesmos calculos. Cada actualização dos pesos\n",
    "# chama-se de epoca.\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# Exemplo\n",
    "# Para 4 variaveis de entrada emprego, filhos, casado, carro, tem-se o seguinte registro 0010.\n",
    "\n",
    "# Codificação (Representação com 2 dimensões da camada de entrada):\n",
    "# A ligação da camada de entrada a camada oculta faz-se atraves de linhas(como se fossem os pesos). Para exemplificação\n",
    "# representou-se a linha sólida por +1 e a linha a tracejado por -1. Na prática estes valores são inicializados aleatoriamente,\n",
    "# como se fossem os pesos.\n",
    "# A ligação do neurónio emprego ao neuronio 1 da camada oculta faz-se através de uma linha sólida.\n",
    "# A ligação do neurónio casado ao neuronio 1 da camada oculta faz-se através de uma linha sólida.\n",
    "# A ligação do neurónio casado ao neuronio 2 da camada oculta faz-se através de uma linha sólida.\n",
    "# A ligação do neurónio carro ao neuronio 2 da camada oculta faz-se através de uma linha sólida.\n",
    "# Todas as restantes ligações fazem-se através de linhas tracejadas.\n",
    "# Para o neurónio 1 da camada oculta tem-se: (0x1)+(0x-1)+(1x1)+(0x-1)=1\n",
    "# Para o neurónio 2 da camada oculta tem-se; (0x-1)+(0x-1)+(1x1)+(0x1)=1\n",
    "\n",
    "# Descodificação:\n",
    "# A ligação da camada de oculta faz-se atraves de linhas(como se fossem os pesos). Para exemplificação\n",
    "# representou-se a linha sólida por +1 e a linha a tracejado por -1. Na prática estes valores são inicializados aleatoriamente,\n",
    "# como se fossem os pesos.\n",
    "# A ligação do neurónio 1 ao neurónio 1 da camada de saida (emprego) faz-se através de 1 linha sólida.\n",
    "# A ligação do neurónio 1 ao neurónio 3 da camada de saida (casado) faz-se através de 1 linha sólida.\n",
    "# A ligação do neurónio 2 ao neurónio 2 da camada de saida (filhos) faz-se através de 1 linha sólida.\n",
    "# A ligação do neurónio 2 ao neurónio 3 da camada de saida (casado) faz-se através de 1 linha sólida.\n",
    "# Para o neurónio 1 da camada de saida(emprego) tem-se: (1x1)+(1x-1) = 0\n",
    "# Para o neurónio 2 da camada de saida(filhos) tem-se: (1x-1)+(1x1) = 0\n",
    "# Para o neurónio 3 da camada de saida(casado) tem-se: (1x1)+(1x1) = 2\n",
    "# Para o neurónio 4 da camada de saida(carro) tem-se: (1x-1)+(1x-1) = -2\n",
    "# Aplicação de uma função de activação, que pode ser a função softmax (que transforma todos os valores menores ou iguais a zero\n",
    "# em zero e os valores maiores a zero em um).\n",
    "# Para o neurónio 1 da camada de saida(emprego) tem-se: (1x1)+(1x-1) = 0 -> 0\n",
    "# Para o neurónio 2 da camada de saida(filhos) tem-se: (1x-1)+(1x1) = 0 -> 0\n",
    "# Para o neurónio 3 da camada de saida(casado) tem-se: (1x1)+(1x1) = 2 -> 1\n",
    "# Para o neurónio 4 da camada de saida(carro) tem-se: (1x-1)+(1x-1) = -2 -> 0\n",
    "\n",
    "# Neste caso na primeira iteração já se obteve os mesmos valores na camada de saida e de entrada. O que significa que o registro\n",
    "# de 4 dimensões 0010 pode ser representado por um registro de 2 dimensões 11, que foi o valor obtido na camada oculta.\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "# Outro tipo de Autoencoders.\n",
    "# Em vez de utilizar o autoencoder para reduzir a dimensionalidade, o autoencoder pode ser utilizado para aumentar a\n",
    "# dimensionalidade. Isto pode ser feito colocando mais neuronios na camada oculta do que na camada de entrada. Pode ser feito\n",
    "# para se ter mais atributos na base de dados, ter uma representação mais detalhada dos dados. \n",
    "# Um dos problemas que pode acontecer é que durante a codificação os valores da camada de entrada podem ser simplesmente \n",
    "# copiados para a camada oculta.\n",
    "# Para solucionar este problema foram criados os seguintes autoencoders \n",
    "\n",
    "# Sparce autoencoder:\n",
    "# Este autoencoder utiliza uma tecnica de regularização para prevenir o overfiting.\n",
    "# Não utiliza todos os neurónios da camada oculta(coloca valores pequenos nos pesos). O sparce autoencoder a cada epoca não \n",
    "# considera alguns neuronios aleatorios da camada oculta. Por exemplo durante o treino, na primeira época não considera o \n",
    "# neuronio 1 da camada oculta. Na segunda época não considera o neuronio 1 e neuronio 4 da camada oculta.\n",
    "\n",
    "# Denoising autoencoder:\n",
    "# Todas as camadas têm o mesmo número de neurónios.\n",
    "# Modifica os valores da camadd de entrada , alterando alguns neurónios aleatoriamente para o valor zero durante o treino para\n",
    "# evitar o overfitting. Ou seja a cada epoca não considera alguns neuronios aleatorios da camada de entrada.\n",
    "# Quando os pesos são actualizados, a camada de saida é comparada com os valores originais para se obter o valor do erro.\n",
    "\n",
    "# Contractive autoencoder\n",
    "# Adiciona uma função de custo quando os pesos são actualizados. Funciona da mesma forma do que os autoencoders tradicionais, \n",
    "# apenas durante o processo de back-propagation para o calculo dos pesos é adicionada uma função custo à função do calculo do\n",
    "# erro. Isto faz com que o modelo se adpte melhor aos dados.\n",
    "\n",
    "# Deep autoencoder(stack autoencoder)\n",
    "# Por exemplo se predende-se reduzir a dimensionalidade de 5 para 2.\n",
    "# Na codificação:\n",
    "# Tem-se a camada de entrada com 5 neurónios\n",
    "# Tem-se de seguida uma camada oculta com 4 neurónios.\n",
    "# Tem-se de seguida uma camada oculta com 3 neurónios.\n",
    "# Tem-se de seguida uma camada oculta com 2 neurónios. A camada com a redução da dimensionalidade pretendida.\n",
    "# Na descodificação:\n",
    "# Tem-se de seguida uma camada oculta com 3 neurónios.\n",
    "# Tem-se de seguida uma camada oculta com 4 neurónios.\n",
    "# Tem-se a camada de saida com 5 neurónios.\n",
    "\n",
    "# Convolution autoencoder\n",
    "# Por exemplo tem-se uma imagem do seguinte tamanho 28x28.\n",
    "# Na codificação:\n",
    "# Camada de entrada: 28x28x1 (1 porque é uma figura)\n",
    "# Camada de convolução(e pooling): 14x14x32 (matriz de pooling obtida por 32 detectores de caracteristicas).\n",
    "# Camada de convolução(e pooling): 7x7x64\n",
    "# Camada de convolução(e pooling): 3x3x128\n",
    "# Camada de flatter: 1152 (3x3x128)\n",
    "# Camada oculta com 10 neurónios. A camada com a redução da dimensionalidade pretendida (neste caso 10 neurónios).\n",
    "# Na descodificação:\n",
    "# Camada de flatter: 1152 (3x3x128)\n",
    "# Camada de convolução(e pooling): 3x3x128\n",
    "# Camada de convolução(e pooling): 7x7x64\n",
    "# Camada de convolução(e pooling): 14x14x32\n",
    "# Camada de saida: 28x28x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, datasets, utils, preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obter os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A base de dados já está dividida em dois conjuntos: treino e teste\n",
    "(X_train, y_train), (X_test,y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60000 imagens para treino\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10000 imagens para teste\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7100303358>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Número zero a preto e branco\n",
    "plt.imshow(X_train[1], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz com os pixeis da imagem com o número zero.\n",
    "# Matriz 28x28 (784 pixeis).\n",
    "pixeis = X_train[1]\n",
    "pixeis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60000 imagens para treino\n",
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformar os dados num vector em que o tensorflow consiga fazer a sua leitura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para redes neurais convolucionais:\n",
    "# 1º parametro: número de imagens\n",
    "# 2º parametro: Altura da imagem (nº de pixeis da imagem na vertical).\n",
    "# 3º parametro: Largura da imagem (nº de pixeis da imagem na horizontal).\n",
    "\n",
    "# Nº de canais de RGB: Como a cor não tem influência neste caso pode-se utilizar a imagem a preto e branco(rgb=1). Quando se \n",
    "# utilizam as imagens a cores (rgb=3) o algoritmo fica mais lento porque aumenta a dimensionalidade dos dados. Uma imagem a \n",
    "# cores tem 3 canais(valores) dentro de cada pixel, porque o pixel é subdividio em vermelho(r), verde(g e azul(b)). Com rgb=1 \n",
    "# tem-se menos dados para processar.\n",
    "\n",
    "# A escala de cinzento possui apenas um canal. Quanto mais próximo de 255 mais claro é o cinzento e quanto mais próximo de \n",
    "# 0 mais escura é a cor.\n",
    "# Nesta escala o Pixel com o valor 255 é o branco e pixel 0 é o preto. Ao redor do número tem-se apenas o valor 0, que indica o \n",
    "# preto(o fundo é preto).\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) # (60000, 28, 28, 1)\n",
    "#X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "# Para redes neurais artificiais:\n",
    "# 1º parametro: número de imagens.\n",
    "# 2º parametro: quantidade de pixeis por imagem. Neste caso tem-se 28x28=784 pixeis.\n",
    "\n",
    "X_train = X_train.reshape((len(X_train), np.prod(X_test.shape[1:]))) # (60000, 784)\n",
    "X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão dos dados do tipo int8 para float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.uint8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como os dados vão ser convertidos para uma escala entre 0 e 1 tem-se de converter os dados de inteiro8 para float32 para que\n",
    "# os valores obtidos depois da normalização não sejam inteiros e consequentemente quase todos 0(por não haver números decimais).\n",
    "\n",
    "#X_test[0,1,1]\n",
    "type(X_test[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversão dos dados em float 32\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização dos pixeis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passar os pixeis para uma escala de 0 a 1 para que o precessamento dos dados seja mais rapido\n",
    "# Isto pode ser feito através da tecnica min max normalization. Como cada pixel ocupa 1 byte e o byte consegue guardar 256\n",
    "# resultados possiveis(ou seja varia entre 0 e 255). \n",
    "# A normalização pode ser feita dividindo o pixel por 255 (tambem se podia utilizar o skit-learn).\n",
    "\n",
    "# Pode-se ver que o valor máximo de um pixel é de 255\n",
    "X_test[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização do conjunto de teste e treino\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação de variaveis em dummys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como se trata de um problema de multiplas classes, cada classe tem de ter a probabilidade de pertencer a um número, ou seja\n",
    "# tem de ser um vector [60000,10] e [10000,10] (exigencia da biblioteca) e não se pode utilizar o LabelEncoder.\n",
    "\n",
    "# número 1: 1 0 0 0 0 0 0 0 0 0\n",
    "# número 2: 0 1 0 0 0 0 0 0 0 0\n",
    "# ....\n",
    "# número 0: 0 0 0 0 0 0 0 0 0 1\n",
    "\n",
    "# tambem se podia utilizar o pd.get_dummies()\n",
    "y_train = utils.to_categorical(y_train, 10)\n",
    "y_test = utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor de compactação = 784/32 = 24.5.Vai-se compactar a imagem 24.5 vezes em relação à original.\n",
    "\n",
    "# Sequential - A informação move-se da camada de entrada, para a camada oculta e de seguida para a camada de saida.\n",
    "\n",
    "#Dense - Camadas densas(cada um dos neurónios é ligado com todos os neuronios da camada seguinte) tambem chamada de rede neural\n",
    "# fully connected\n",
    "\n",
    "# 1ª Camada Oculta(e definição da camada de entrada) \n",
    "# units: Número de neuronios da 1ª camada oculta. Definir qual a redução de dimensionalidade que se quer. Neste caso pretende-se\n",
    "# passar de 784 variaveis de entrada para 32.\n",
    "# Função de Activação: Normalmente escolhe-se a reLu para deep learning porque obtem-se melhores resultados do que com uma\n",
    "# função sigmoide ou tangente hiperbolica. \n",
    "# Kernel initializer: Como se inicializam os pesos.\n",
    "# input_dim: quantos atributos existem na camada de entrada (neste caso são 784 variaveis de entrada). Este parâmetro só é \n",
    "# necessário para a primeira camada oculta. Com este parâmetro activo não é necessário colocar explicitamente a camada de \n",
    "# entrada porque na 1ª camada oculta está-se a dizer o numero de neurónios da camada de entrada.\n",
    "\n",
    "# Camada de Saida\n",
    "# units: Número de neurónios da camada de saida. Igual ao número de neurónios da camada de entrada.\n",
    "# Função de activação: Sigmoid porque é um problema de classificação binário que varia entre 0 e 1, e esta função retorna sempre\n",
    "# um valor entre 0 e 1. Pode-se utilizar a sigmoid porque foi feita a normalização dos pixeis entre 0 e 1. Tambem se utiliza\n",
    "# bastantes vezes a função sigmoid.\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(units=32, activation=\"relu\", input_dim=784)) # Camada oculta e difinição da camada de entrada (codificação da camada de entrada: redução da dimensionalidade).\n",
    "model.add(layers.Dense(units=784, activation=\"sigmoid\")) # Camada de Saida(descodificação da camada oculta).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 784x32+32(bies na camada oculta)=25120 parâmetros(pesos)\n",
    "# 32x784+784(bies na camada de entrada)=25872 parâmetros\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilação e Ajuste do modelo autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: adam é uma optimização da descida do gradiente estócastico (usado para encontrar os melhores valores dos pesos).\n",
    "# É o que melhor se adapta à maioria dos casos.\n",
    "# loss = função de perda binary_crossentropy é a mais utilizada para classificação binária. Mede o quão afastada está a previsão\n",
    "# do seu valor real(0 ou 1) para cada classe e faz a média desses erros(desvios) para obter o custo(loss). Como utiliza o\n",
    "# logaritmo tem uma penalização maior quando existe uma classificação errada.\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1417 - binary_accuracy: 0.8076 - val_loss: 0.1314 - val_binary_accuracy: 0.8084\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.1266 - binary_accuracy: 0.8100 - val_loss: 0.1194 - val_binary_accuracy: 0.8101\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.1165 - binary_accuracy: 0.8115 - val_loss: 0.1111 - val_binary_accuracy: 0.8114\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.1093 - binary_accuracy: 0.8125 - val_loss: 0.1052 - val_binary_accuracy: 0.8120\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1043 - binary_accuracy: 0.8131 - val_loss: 0.1010 - val_binary_accuracy: 0.8124\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.1008 - binary_accuracy: 0.8135 - val_loss: 0.0981 - val_binary_accuracy: 0.8128\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0984 - binary_accuracy: 0.8137 - val_loss: 0.0962 - val_binary_accuracy: 0.8130\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0968 - binary_accuracy: 0.8139 - val_loss: 0.0951 - val_binary_accuracy: 0.8130\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0958 - binary_accuracy: 0.8140 - val_loss: 0.0942 - val_binary_accuracy: 0.8132\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0952 - binary_accuracy: 0.8140 - val_loss: 0.0937 - val_binary_accuracy: 0.8132\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0947 - binary_accuracy: 0.8141 - val_loss: 0.0933 - val_binary_accuracy: 0.8132\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0944 - binary_accuracy: 0.8141 - val_loss: 0.0930 - val_binary_accuracy: 0.8132\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0941 - binary_accuracy: 0.8141 - val_loss: 0.0927 - val_binary_accuracy: 0.8132\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0939 - binary_accuracy: 0.8142 - val_loss: 0.0926 - val_binary_accuracy: 0.8132\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0938 - binary_accuracy: 0.8142 - val_loss: 0.0924 - val_binary_accuracy: 0.8132\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0936 - binary_accuracy: 0.8142 - val_loss: 0.0925 - val_binary_accuracy: 0.8132\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0935 - binary_accuracy: 0.8142 - val_loss: 0.0922 - val_binary_accuracy: 0.8133\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0934 - binary_accuracy: 0.8142 - val_loss: 0.0922 - val_binary_accuracy: 0.8133\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0933 - binary_accuracy: 0.8142 - val_loss: 0.0921 - val_binary_accuracy: 0.8133\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0933 - binary_accuracy: 0.8142 - val_loss: 0.0921 - val_binary_accuracy: 0.8133\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0932 - binary_accuracy: 0.8142 - val_loss: 0.0919 - val_binary_accuracy: 0.8133\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0932 - binary_accuracy: 0.8142 - val_loss: 0.0920 - val_binary_accuracy: 0.8133\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0931 - binary_accuracy: 0.8142 - val_loss: 0.0919 - val_binary_accuracy: 0.8132\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0931 - binary_accuracy: 0.8142 - val_loss: 0.0919 - val_binary_accuracy: 0.8133\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0931 - binary_accuracy: 0.8142 - val_loss: 0.0918 - val_binary_accuracy: 0.8133\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0930 - binary_accuracy: 0.8142 - val_loss: 0.0918 - val_binary_accuracy: 0.8132\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0930 - binary_accuracy: 0.8142 - val_loss: 0.0918 - val_binary_accuracy: 0.8133\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0930 - binary_accuracy: 0.8142 - val_loss: 0.0918 - val_binary_accuracy: 0.8134\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0929 - binary_accuracy: 0.8142 - val_loss: 0.0918 - val_binary_accuracy: 0.8133\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0929 - binary_accuracy: 0.8142 - val_loss: 0.0917 - val_binary_accuracy: 0.8133\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.0929 - binary_accuracy: 0.8142 - val_loss: 0.0917 - val_binary_accuracy: 0.8133\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0929 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0929 - binary_accuracy: 0.8142 - val_loss: 0.0917 - val_binary_accuracy: 0.8133\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.0928 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0928 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0928 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0928 - binary_accuracy: 0.8142 - val_loss: 0.0917 - val_binary_accuracy: 0.8133\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0928 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0927 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0927 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0927 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0927 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8134\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0927 - binary_accuracy: 0.8142 - val_loss: 0.0916 - val_binary_accuracy: 0.8133\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0927 - binary_accuracy: 0.8142 - val_loss: 0.0915 - val_binary_accuracy: 0.8133\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0927 - binary_accuracy: 0.8142 - val_loss: 0.0915 - val_binary_accuracy: 0.8133\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0926 - binary_accuracy: 0.8142 - val_loss: 0.0915 - val_binary_accuracy: 0.8133\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0926 - binary_accuracy: 0.8142 - val_loss: 0.0915 - val_binary_accuracy: 0.8133\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0926 - binary_accuracy: 0.8142 - val_loss: 0.0915 - val_binary_accuracy: 0.8133\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0926 - binary_accuracy: 0.8142 - val_loss: 0.0915 - val_binary_accuracy: 0.8133\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0926 - binary_accuracy: 0.8142 - val_loss: 0.0915 - val_binary_accuracy: 0.8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x71659c7940>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size\n",
    "# batch_size: 256, calcula-se o erro de um conjunto de 256 dados e actualizam-se os pesos. Quanto mais alto este valor, mais\n",
    "# rapido são executados os calculos.\n",
    "\n",
    "# epochs: Quantas vezes(iterações) se ajustam os valores dos pesos para cada conjunto de dados(batch). Ou seja quantas vezes se\n",
    "# faz o treino dos dados com os valores dos pesos a serem melhorados.\n",
    "\n",
    "# 1ª epoca- calcula o erro para os primeiros 256 dados e actualiza os pesos. calcula o o erro para os 256 dados seguintes e\n",
    "# actualiza os pesos. E assim sucessivamente.\n",
    "# 2ª epoca- calcula o erro para os primeiros 256 dados(com os pesos da 1ª epoca) e actualiza os pesos. calcula o erro para \n",
    "# os 256 dados seguintes(com os pesos da 1ª epoca) e actualiza os pesos. E assim sucessivamente.\n",
    "\n",
    "# Validation data: Avaliação do modelo utilizando a base de dados de teste (val_accuracy).\n",
    "\n",
    "# Em vez do y_train utiliza-se o X_train em seu lugar. \n",
    "# Está-se a fazer a codificação da camada de entrada no nº de neurónios da camada oculta.\n",
    "# De seguida faz-se a descodificação da camada oculta na camada de saida.\n",
    "# De seguida compara-se a camada de saida com a camada de entrada(e não com o y_train).\n",
    "# Calcula-se o erro e ajustam-se os pesos utilizando a tecnica back-propagation.\n",
    "# Repetem-se os passos anteriores para o número de épocas definido.\n",
    "\n",
    "# Ajustar os dados de entrada de treino aos dados de saida de treino para treinar o modelo\n",
    "model.fit(X_train, X_train, batch_size=256, epochs=50, validation_data=(X_test,X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Autoencoder para redução da dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original size: Camada de entrada com as dimensões originais da imagem.\n",
    "# encoded size: Camada com a redução da dimensionalidade aplicada através da codificação(camada zero do modelo do autoencoder).\n",
    "\n",
    "original_size = layers.Input(shape=784,) # Camada de entrada.\n",
    "encoded_size = layers.Dense(units=32, activation=\"relu\", input_dim=784)\n",
    "#encoded_size = model.layers[0] # Camada oculta(codificação da camada de entrada: redução da dimensionalidade)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com o modelo de autoencoder model fez-se a codificação e descodificação. Ou seja reduziu-se a dimensionalidade das imagens e\n",
    "# depois voltou a transformar-se as imagens na dimensão original.\n",
    "# Com o modelo encoder apenas se faz a codificação. Ou seja apenas se reduz a dimensionalidade das imagens.\n",
    "\n",
    "# Modelo para fazer a redução de dimensionalidade.\n",
    "\n",
    "# Model utiliza-se quando se quer criar manualmente a rede neural.\n",
    "# O modelo encoder recebe o original size e transforma no encoded_size.\n",
    "\n",
    "# Model(camada de entrada, estrutura da rede).\n",
    "# estrutura da rede: a camada encoded_size segue-se à camada original_size.\n",
    "encoder = models.Model(original_size, encoded_size(original_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                25120     \n",
      "=================================================================\n",
      "Total params: 25,120\n",
      "Trainable params: 25,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Transforma de 784 dimensões para 32 dimensões.\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vai-se utilizar o X_train_codificado(imagens com redução de dimensionalidade) para treinar um modelo de rede neural e \n",
    "# comparar com resultados obtidos utilizando o X_train(imagens originais).\n",
    "\n",
    "# Imagens de teste codificadas(com redução de dimensionalidade de 784 para 32).\n",
    "X_train_codificado = encoder.predict(X_train)\n",
    "X_train_codificado[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vai-se utilizar o X_test_codificado(imagens com redução de dimensionalidade) para avaliar o um modelo de rede neural treinado\n",
    "# utilizando o X_train_codificado e comparar com resultados obtidos utilizando o X_test(imagens originais).\n",
    "\n",
    "# Imagens de teste codificadas(com redução de dimensionalidade de 784 para 32).\n",
    "X_test_codificado = encoder.predict(X_test)\n",
    "X_test_codificado[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de rede neural artificial sem redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential - uma cada apos a outra\n",
    "# dense - camadas fully conected, cada neuronio esta conectado a todos os neuronios da camada seguinte\n",
    "\n",
    "# 1ª Camada Oculta(e definição da camada de entrada) \n",
    "# Neuronios: normalmente escolhe-se o número de neuronios igual ao número de variaveis de entrada mais o número de variaveis de \n",
    "# saida divididos por 2 ((784+10)/2=397) para começar a modelar a rede. \n",
    "# Função de Activação: Normalmente escolhe-se a reLu para deep learning porque obtem-se melhores resultados do que com uma\n",
    "# função sigmoide ou tangente hiperbolica. \n",
    "# input_dim: quantos atributos existem na camada de entrada (neste caso são 784 variaveis de entrada). Este parâmetro só é \n",
    "# necessário para a primeira camada oculta.\n",
    "\n",
    "# 2ª Camada Oculta\n",
    "# Na segunda camada oculta em geral coloca-se a mesma quantidade de neurónios do que na primeira.\n",
    "\n",
    "# Camada de Saida\n",
    "# Neuronios: Igual ao número de saidas possiveis, neste caso são 3 classes.\n",
    "# Função de activação: softmax é a função utilizada para problemas de classificação multiclasse. Para se obter a probabilidade\n",
    "# para cada uma das classes (por ex. para um neurónio tem-se a probabilidade de 10% de ser nº1, 80% nº2 e 10% de ser nº3. \n",
    "# A classe atribuida será aquela que tem maior probabilidade, neste ex. a iris verginica).\n",
    "\n",
    "model_non_reduced = models.Sequential()\n",
    "model_non_reduced.add(layers.Dense(units=397, activation=\"relu\", input_dim=784))\n",
    "model_non_reduced.add(layers.Dense(units=397, activation=\"relu\"))\n",
    "model_non_reduced.add(layers.Dense(units=10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilação e Ajuste do modelo ANN sem redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: adam é uma optimização da descida do gradiente estócastico (usado para encontrar os melhores valores dos pesos).\n",
    "# É o que melhor se adapta à maioria dos casos.\n",
    "\n",
    "# loss = função de perda categorical_crossentropy é a mais utilizada para classificação multiclasse.\n",
    "\n",
    "model_non_reduced.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0022 - categorical_accuracy: 0.9995 - val_loss: 0.0862 - val_categorical_accuracy: 0.9845\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0016 - categorical_accuracy: 0.9997 - val_loss: 0.0892 - val_categorical_accuracy: 0.9847egorical_accu\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0012 - categorical_accuracy: 0.9998 - val_loss: 0.0898 - val_categorical_accuracy: 0.9846\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 8.6610e-04 - categorical_accuracy: 0.9999 - val_loss: 0.0921 - val_categorical_accuracy: 0.9850\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 7.0428e-04 - categorical_accuracy: 0.9999 - val_loss: 0.0933 - val_categorical_accuracy: 0.9851\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 5.7542e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0956 - val_categorical_accuracy: 0.9851\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 5.0193e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0968 - val_categorical_accuracy: 0.9847\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 4.4837e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0971 - val_categorical_accuracy: 0.9850\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 4.1169e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0979 - val_categorical_accuracy: 0.9852\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 3.7959e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0986 - val_categorical_accuracy: 0.9855\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 3.5797e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1003 - val_categorical_accuracy: 0.9855\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 3.3964e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1009 - val_categorical_accuracy: 0.9857\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 3.2767e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1016 - val_categorical_accuracy: 0.9858\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 3.1736e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1020 - val_categorical_accuracy: 0.9860\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 3.0976e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1031 - val_categorical_accuracy: 0.9862\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 3.0233e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1042 - val_categorical_accuracy: 0.9862\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.9746e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1043 - val_categorical_accuracy: 0.9861\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.9308e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1053 - val_categorical_accuracy: 0.9860\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.8922e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1058 - val_categorical_accuracy: 0.9862\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 2.8619e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1066 - val_categorical_accuracy: 0.9864\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 2.8373e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1077 - val_categorical_accuracy: 0.9862\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 2.8144e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1081 - val_categorical_accuracy: 0.9864\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.7951e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1088 - val_categorical_accuracy: 0.9865\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.7800e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1097 - val_categorical_accuracy: 0.9865\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.7677e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1104 - val_categorical_accuracy: 0.9865\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 2.7563e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1101 - val_categorical_accuracy: 0.9864\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.7477e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1109 - val_categorical_accuracy: 0.9865\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 2.7390e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1118 - val_categorical_accuracy: 0.9860\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 2.7316e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1123 - val_categorical_accuracy: 0.9862\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.7261e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1131 - val_categorical_accuracy: 0.9862\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.7213e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1128 - val_categorical_accuracy: 0.9865\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.7168e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1141 - val_categorical_accuracy: 0.9863\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.7131e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1146 - val_categorical_accuracy: 0.9866\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.7099e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1147 - val_categorical_accuracy: 0.9863\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.7071e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1154 - val_categorical_accuracy: 0.9866\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.7047e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1154 - val_categorical_accuracy: 0.9865\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.7023e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1168 - val_categorical_accuracy: 0.9867\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 2.7008e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1170 - val_categorical_accuracy: 0.9865\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.6990e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1179 - val_categorical_accuracy: 0.9865\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.6977e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1181 - val_categorical_accuracy: 0.9865\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.6966e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1184 - val_categorical_accuracy: 0.9866\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 2.6954e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1193 - val_categorical_accuracy: 0.9868\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.6946e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1198 - val_categorical_accuracy: 0.9870\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 2.6938e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1199 - val_categorical_accuracy: 0.9867\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 2.6930e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1206 - val_categorical_accuracy: 0.9865\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 2.6924e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1211 - val_categorical_accuracy: 0.9866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.6918e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1213 - val_categorical_accuracy: 0.9865\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 2.6913e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1220 - val_categorical_accuracy: 0.9864\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.6909e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1225 - val_categorical_accuracy: 0.9865\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.6905e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1231 - val_categorical_accuracy: 0.9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x71077b3048>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size\n",
    "# batch_size: 256, calcula-se o erro de um conjunto de 256 dados e actualizam-se os pesos. Quanto mais alto este valor, mais\n",
    "# rapido são executados os calculos.\n",
    "\n",
    "# epochs: Quantas vezes(iterações) se ajustam os valores dos pesos para cada conjunto de dados(batch). Ou seja quantas vezes se\n",
    "# faz o treino dos dados com os valores dos pesos a serem melhorados.\n",
    "\n",
    "# 1ª epoca- calcula o erro para os primeiros 256 dados e actualiza os pesos. calcula o o erro para os 256 dados seguintes e\n",
    "# actualiza os pesos. E assim sucessivamente.\n",
    "# 2ª epoca- calcula o erro para os primeiros 256 dados(com os pesos da 1ª epoca) e actualiza os pesos. calcula o erro para \n",
    "# os 256 dados seguintes(com os pesos da 1ª epoca) e actualiza os pesos. E assim sucessivamente.\n",
    "\n",
    "# Validation data: Avaliação do modelo utilizando a base de dados de teste (val_accuracy).\n",
    "\n",
    "# Ajustar os dados de entrada de treino aos dados de saida de treino para treinar o modelo\n",
    "model_non_reduced.fit(X_train, y_train, batch_size=256, epochs=50, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de rede neural artificial com redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de rede neural artificial \n",
    "\n",
    "# 1ª Camada Oculta(e definição da camada de entrada) \n",
    "# Neuronios: normalmente escolhe-se o número de neuronios igual ao número de variaveis de entrada mais o número de variaveis de \n",
    "# saida divididos por 2 ((32+10)/2=21) para começar a modelar a rede. \n",
    "# Função de Activação: Normalmente escolhe-se a reLu para deep learning porque obtem-se melhores resultados do que com uma\n",
    "# função sigmoide ou tangente hiperbolica. \n",
    "# input_dim: quantos atributos existem na camada de entrada (neste caso são 32 variaveis de entrada). Este parâmetro só é \n",
    "# necessário para a primeira camada oculta.\n",
    "\n",
    "model_reduced = models.Sequential()\n",
    "model_reduced.add(layers.Dense(units=21, activation=\"relu\", input_dim=32))\n",
    "model_reduced.add(layers.Dense(units=21, activation=\"relu\"))\n",
    "model_reduced.add(layers.Dense(units=10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilação e Ajuste do modelo ANN com redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilação do modelo\n",
    "model_reduced.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7446 - categorical_accuracy: 0.7574 - val_loss: 0.7167 - val_categorical_accuracy: 0.7652\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7411 - categorical_accuracy: 0.7588 - val_loss: 0.7125 - val_categorical_accuracy: 0.7627\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7380 - categorical_accuracy: 0.7599 - val_loss: 0.7083 - val_categorical_accuracy: 0.7648\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7350 - categorical_accuracy: 0.7604 - val_loss: 0.7062 - val_categorical_accuracy: 0.7668\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7309 - categorical_accuracy: 0.7625 - val_loss: 0.7010 - val_categorical_accuracy: 0.7678\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7285 - categorical_accuracy: 0.7628 - val_loss: 0.7009 - val_categorical_accuracy: 0.7688\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7249 - categorical_accuracy: 0.7640 - val_loss: 0.6979 - val_categorical_accuracy: 0.7680\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7226 - categorical_accuracy: 0.7639 - val_loss: 0.6981 - val_categorical_accuracy: 0.7686\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7206 - categorical_accuracy: 0.7646 - val_loss: 0.6931 - val_categorical_accuracy: 0.7706\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.7169 - categorical_accuracy: 0.7674 - val_loss: 0.6940 - val_categorical_accuracy: 0.7688\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7142 - categorical_accuracy: 0.7680 - val_loss: 0.6902 - val_categorical_accuracy: 0.7700\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7120 - categorical_accuracy: 0.7674 - val_loss: 0.6888 - val_categorical_accuracy: 0.7725\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.7094 - categorical_accuracy: 0.7689 - val_loss: 0.6852 - val_categorical_accuracy: 0.7735\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.7067 - categorical_accuracy: 0.7688 - val_loss: 0.6822 - val_categorical_accuracy: 0.7737\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.7042 - categorical_accuracy: 0.7714 - val_loss: 0.6811 - val_categorical_accuracy: 0.7746\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.7026 - categorical_accuracy: 0.7705 - val_loss: 0.6803 - val_categorical_accuracy: 0.7745\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.7003 - categorical_accuracy: 0.7712 - val_loss: 0.6828 - val_categorical_accuracy: 0.7731\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6977 - categorical_accuracy: 0.7725 - val_loss: 0.6770 - val_categorical_accuracy: 0.7764\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6958 - categorical_accuracy: 0.7736 - val_loss: 0.6778 - val_categorical_accuracy: 0.7743\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6941 - categorical_accuracy: 0.7735 - val_loss: 0.6770 - val_categorical_accuracy: 0.7790\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.6919 - categorical_accuracy: 0.7749 - val_loss: 0.6717 - val_categorical_accuracy: 0.7786\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6897 - categorical_accuracy: 0.7753 - val_loss: 0.6685 - val_categorical_accuracy: 0.7760\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.6884 - categorical_accuracy: 0.7757 - val_loss: 0.6723 - val_categorical_accuracy: 0.7764\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6870 - categorical_accuracy: 0.7760 - val_loss: 0.6680 - val_categorical_accuracy: 0.7754\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6848 - categorical_accuracy: 0.7763 - val_loss: 0.6661 - val_categorical_accuracy: 0.7778\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6831 - categorical_accuracy: 0.7775 - val_loss: 0.6654 - val_categorical_accuracy: 0.7796\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6823 - categorical_accuracy: 0.7772 - val_loss: 0.6637 - val_categorical_accuracy: 0.7776\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6806 - categorical_accuracy: 0.7783 - val_loss: 0.6625 - val_categorical_accuracy: 0.7790\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6791 - categorical_accuracy: 0.7782 - val_loss: 0.6635 - val_categorical_accuracy: 0.7784\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6787 - categorical_accuracy: 0.7786 - val_loss: 0.6619 - val_categorical_accuracy: 0.7779\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6766 - categorical_accuracy: 0.7794 - val_loss: 0.6576 - val_categorical_accuracy: 0.7802\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6754 - categorical_accuracy: 0.7800 - val_loss: 0.6581 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6737 - categorical_accuracy: 0.7801 - val_loss: 0.6560 - val_categorical_accuracy: 0.7811\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6723 - categorical_accuracy: 0.7807 - val_loss: 0.6592 - val_categorical_accuracy: 0.7786\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6711 - categorical_accuracy: 0.7810 - val_loss: 0.6539 - val_categorical_accuracy: 0.7814\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6703 - categorical_accuracy: 0.7813 - val_loss: 0.6528 - val_categorical_accuracy: 0.7815\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6689 - categorical_accuracy: 0.7813 - val_loss: 0.6515 - val_categorical_accuracy: 0.7823\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6680 - categorical_accuracy: 0.7814 - val_loss: 0.6528 - val_categorical_accuracy: 0.7826\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6668 - categorical_accuracy: 0.7821 - val_loss: 0.6497 - val_categorical_accuracy: 0.7824\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6663 - categorical_accuracy: 0.7820 - val_loss: 0.6481 - val_categorical_accuracy: 0.7826\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6645 - categorical_accuracy: 0.7828 - val_loss: 0.6474 - val_categorical_accuracy: 0.7828\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6638 - categorical_accuracy: 0.7829 - val_loss: 0.6479 - val_categorical_accuracy: 0.7825\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6627 - categorical_accuracy: 0.7831 - val_loss: 0.6448 - val_categorical_accuracy: 0.7835\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6615 - categorical_accuracy: 0.7831 - val_loss: 0.6448 - val_categorical_accuracy: 0.7839\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6605 - categorical_accuracy: 0.7832 - val_loss: 0.6415 - val_categorical_accuracy: 0.7870\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6600 - categorical_accuracy: 0.7844 - val_loss: 0.6433 - val_categorical_accuracy: 0.7851\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6590 - categorical_accuracy: 0.7840 - val_loss: 0.6425 - val_categorical_accuracy: 0.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.6582 - categorical_accuracy: 0.7845 - val_loss: 0.6449 - val_categorical_accuracy: 0.7853\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6582 - categorical_accuracy: 0.7848 - val_loss: 0.6400 - val_categorical_accuracy: 0.7847\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.6566 - categorical_accuracy: 0.7851 - val_loss: 0.6410 - val_categorical_accuracy: 0.7872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7107b10eb8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustar os dados de entrada de treino codificados aos dados de saida de treino codificados para treinar o modelo\n",
    "model_reduced.fit(X_train_codificado, y_train, batch_size=256, epochs=50, validation_data=(X_test_codificado,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
