{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "1. [Tensorflow 2.x vs tensorflow 1.x](#tensorflow)  \n",
    "2. [Modelos Supervisionados](#supervisionados)  \n",
    "2.1 [Redes Neurais Artificiais](#artificiais)    \n",
    "2.2 [Redes Neurais Convolucionais](#convolucionais)    \n",
    "2.3 [Redes Neurais Recorrentes](#recorrentes)    \n",
    "3. [Modelos Não Supervisionados](#naosupervisionados)  \n",
    "3.1 [Mapas Auto-Organizáveis](#mapas)  \n",
    "3.2 [Boltzmann machines](#boltzmann)  \n",
    "3.3 [Autoencoders](#autoencoders)  \n",
    "3.4 [Redes Adversariais Generativas(GANs)](#gans)\n",
    "4. [Bibliografia](#bibliografia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2.x vs Tensorflow 1.x <a name=\"tensorflow\"></a>\n",
    "\n",
    "- O tensorflow 2.0 utiliza o eager execution por defeito. O eager execution não suporta o tf.placeholders e tf.Variable.     \n",
    "A eager execution foi lancado no tensorflow 1.7 mas tinha de ser activada manualmente e tinha-se de executar uma função para transformar o grafo da Eager Execution em um grafo executável pelo TF.                \n",
    "- No tensorflow 2.0 definiu-se a utilização da API da Keras como método preferencial para a construção de modelos de redes neurais.        \n",
    "- No tensorflow 2.0 foi limpo o código duplicado. Por exemplo o módulo tf.contrib foi removido.    \n",
    "- No tensorflow 2.0 deixa de haver variáveis globais. Quando se chamava uma tf.Variable(), por exemplo, ela era colocada no grafo default e continuaria lá mesmo se perdesse a referência Python que apontava para ela.      \n",
    "- O tensorflow 2.0 utiliza funções e não sessões. A session.run() funcionava quase como uma chamada de função. Eram especificadas as entradas e a função a ser chamada, e recebia-se de volta o conjunto de saídas.   \n",
    "No tensorflow 2.0 pode-se usar tf.function(),com a anotação @tf.function, para marcar uma função a ser compilada em tempo de execução (JIT compilation). Dessa forma, o TensorFlow vai rodá-la em um único grafo.        \n",
    "\n",
    "## Exemplos      \n",
    "- Não é necessário definir as variáveis com tf.Variable().  \n",
    "  one = tf.constant(1)    \n",
    "  Por exemplo ten=tf.Variable(10)      \n",
    "- Não é necessário inicializar as variáveis com tf.global_variables_initializer().      \n",
    "  sum = tf.add(ten,one)    \n",
    "  update = tf.assign(ten,sum)    \n",
    "  init_op = tf.global_variables_initializer()    \n",
    "- Não é necessário criar a sessão com tf.session() para executar.      \n",
    "  sess = tf.Session()    \n",
    "- Não é necessário executar o grafo/Avaliar o nó com sess.run().    \n",
    "  sess.run(init_op)    \n",
    "  print(sess.run(update))  \n",
    "    \n",
    "  \n",
    "- Não é necessário definir placeholders(os placeholders não são iniciados com valores - ao contrario das constantes e variavies).    \n",
    "  Por exemplo a = tf.placeholder(tf.float32)      \n",
    "  b = a*2      \n",
    "  sess = tf.Session()     \n",
    "  sess.run(b, feed_dict={a:3.0})      \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Supervisionados <a name=\"supervisionados\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/ap.png\" alt=\"Drawing\" style=\"width: 450px;height: 400px;\"/> </td> \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais Artificiais <a name=\"artificiais\"></a>\n",
    "\n",
    "São redes neurais utilizadas para problemas de classificação e regressão.   \n",
    "Tambem podem ser utilizadas para problemas de visão computacional, transformando a imagem(que é uma matriz de pixeis) num vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron de uma camada\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron.png\" alt=\"Drawing\" style=\"width: 350px;height: 300px;\"/> </td> \n",
    "</tr></table>\n",
    "\n",
    "O perceptron é o tipo mais simples de rede neural feedforward: um classificador linear.  \n",
    "O perceptron de uma camada é um tipo de rede neural com apenas camada de entrada e camada de saida.    \n",
    "\n",
    "### Redes Neurais Feedforward:\n",
    "Nas redes neurais feedforward os dados fluem em uma única direção, isto é, eles não voltam para camadas mais atrás nem dão voltas na mesma camada. Os dados percorrem a rede em uma única direção, da entrada para a saída.  \n",
    "\n",
    "O perceptron é um classificador binário que mapeia sua entrada x (um vetor de valor real) para um valor de saída f(x) (um valor binário simples). A função f(x) utilizada é a função step.  \n",
    "Se o valor da função soma for maior ou igual do que 0 passa a 1.  \n",
    "Se o valor da função soma for menor do que 0 passa a 0.  \n",
    "Esses valores podem ser alterados. Por exemplo:  \n",
    "Se o valor da função soma for maior ou igual do que 1 passa a 1.  \n",
    "Se o valor da função soma for menor do que 1 passa a 0.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/percep.png\" alt=\"Drawing\" style=\"width: 250px;height: 100px;\"/> </td>   \n",
    "<td> <img src=\"fotos/percep1.png\" alt=\"Drawing\" style=\"width: 250px;height: 100px;\"/> </td>      \n",
    "</tr></table>\n",
    "\n",
    "Onde w é um vetor de peso real e w.x é o produto escalar (que computa uma soma com pesos) e b é o viés (do inglês \"bias\"), um termo constante que não depende de qualquer valor de entrada.  \n",
    "De seguida tem-se dois exemplos de perceptrons de uma camada, onde se multiplicam os dados de entrada pelos pesos para obter\n",
    "a função soma. De seguida aplica-se a função step à função soma.  \n",
    "Usou-se os seguintes degraus da função step:  \n",
    "Se o valor da função soma for maior do que 0 passa a 1.    \n",
    "Se o valor da função soma for menor ou igual a 0 passa a 0.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron1.png\" alt=\"Drawing\" style=\"width: 450px;height: 300px;\"/> </td> \n",
    "<td> <img src=\"fotos/perceptron2.png\" alt=\"Drawing\" style=\"width: 450px;height: 300px;\"/> </td>     \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron de uma camada para prever o valor da operação and."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/and.png\" alt=\"Drawing\" style=\"width: 650px;height: 200px;\"/> </td>    \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Operador and:  \n",
    "if(x1 and x2)==False -> class is False  \n",
    "if(x1==False) and (x2==True) -> class is False  \n",
    "if(x1==True) and (x2==False) -> class is False  \n",
    "if(x1 and x2)==True -> class is True  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron4.png\" alt=\"Drawing\" style=\"width: 650px;height: 400px;\"/> </td>    \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atribuindo o valor 0 aos pesos para inicializar a rede.     \n",
    "Usando a step function como função de activação e definindo os seguintes degraus:   \n",
    "Se o valor da função soma for maior do que 0 passa a 1.    \n",
    "Se o valor da função soma for menor ou igual a 0 passa a 0.      \n",
    "\n",
    "##### Calculo da função de activação:\n",
    "1º Registro: x1.w1+x2.w2 = 0x0+0x0=0, função soma = 0, função de activação(função step) = 0  \n",
    "2º Registro: x1.w1+x2.w2 = 0x0+1x0=0, função soma = 0, função de activação(função step) = 0  \n",
    "3º Registro: x1.w1+x2.w2 = 1x0+0x0=0, função soma = 0, função de activação(função step) = 0  \n",
    "4º Registro: x1.w1+x2.w2 = 1x0+1x0=0, função soma = 0, função de activação(função step) = 0  \n",
    "\n",
    "##### Calculo do erro(valor real-valor previsto) : \n",
    "1º Registro: classe - valor saída da rede = 0-0=0  \n",
    "2º Registro: classe - valor saída da rede = 0-0=0  \n",
    "3º Registro: classe - valor saída da rede = 0-0=0  \n",
    "4º Registro: classe - valor saída da rede = 1-0=1  \n",
    "O erro é 0 para os primeiro 3 registros e 1 para o 4º. A taxa de acerto é de 75%.  \n",
    "\n",
    "##### Calculo dos pesos da proxima iteração :  \n",
    "peso(n + 1) = peso(n) + (taxaAprendizagem* entrada * erro)      \n",
    "1º Registro: w1 = 0 + (0.1 * 0 * 0) = 0  \n",
    "             w2 = 0 + (0.1 * 0 * 0) = 0  \n",
    "2º Registro: w1 = 0 + (0.1 * 0 * 0) = 0  \n",
    "             w2 = 0 + (0.1 * 1 * 0) = 0  \n",
    "3º Registro: w1 = 0 + (0.1 * 1 * 0) = 0  \n",
    "             w2 = 0 + (0.1 * 0 * 0) = 0       \n",
    "4º Registro: w1 = 0 + (0.1 * 1 * 0) = 0.1  \n",
    "             w2 = 0 + (0.1 * 1 * 0) = 0.1                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron6.png\" alt=\"Drawing\" style=\"width: 650px;height: 400px;\"/> </td>    \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculo da dunção de activação com os novos valores dos pesos :   \n",
    "1º Registro: x1.w1+x2.w2 = 0x0+0x0=0, função soma = 0, função de activação(função step) = 0    \n",
    "2º Registro: x1.w1+x2.w2 = 0x0+1x0=0, função soma = 0, função de activação(função step) = 0    \n",
    "3º Registro: x1.w1+x2.w2 = 1x0+0x0=0, função soma = 0, função de activação(função step) = 0    \n",
    "4º Registro: x1.w1+x2.w2 = 1x0.1+1x0.1=0, função soma = 0.2, função de activação(função step) = 1    \n",
    "    \n",
    "##### Calculo do erro(valor real-valor previsto) :     \n",
    "1º Registro: classe - valor saída da rede = 0-0=0    \n",
    "2º Registro: classe - valor saída da rede = 0-0=0    \n",
    "3º Registro: classe - valor saída da rede = 0-0=0    \n",
    "4º Registro: classe - valor saída da rede = 1-1=0    \n",
    "O erro é 0 para os 4 registros e 1 para o 4º. A taxa de acerto é de 100%.   \n",
    "Não é necessária mais nenhuma iteração porque o valor previsto é igual ao valor real.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron de várias camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron7.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>    \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na figura acima vê-se que a representação gráfica dos operadores and e or são linearmente separaveis. Mas a representação\n",
    "do operador xor não é, ou seja, não é possivel traçar uma linha que separe os dois resultados possiveis(0 ou 1).\n",
    "Pode-se utilizar o perceptron de uma camada para resolver problemas linearmente separaveis. Mesmo que se utilize uma\n",
    "função de activação não linear, como a sigmoid, o perceptron de uma camada só produz resultados lineares.  \n",
    "\n",
    "Mas para problemas mais complexos, não lineamente separaveis, é necessário utilizar uma estrutura mais complexa, que intruduza\n",
    "a não lineariedade ao problema.    \n",
    "As redes neurais com camadas oculta conseguem classificar problemas não linearmente separaveis. Esses problemas podem ser resolvidos com a introdução de camadas ocultas e utilizando como função de activação a step function.    \n",
    "Mas como o algoritmo utilizado para treinar(actualizar os pesos) as redes neurais de multicamadas é geralmente o backpropagation, o que requer que a função de activação seja derivavel. Porque o algoritmo usa a derivada da função de activação como multiplicador.  \n",
    "Como a derivada da step function não é derivavel para x=0 e a derivada é zero para os restantes valores, significa que a descida do gradiente não fará progressos na actualização dos pesos e a actualização dos pesos por backpropagation não irá resultar.     \n",
    "Por isso utilizam-se normalmente as funções sigmoid e tangente hyperbólica como funções de activação, já que as suas derivadas são facilmente demonstráveis.    \n",
    "\n",
    "Ao se adicionar mais layers ocultas/mais neuronios por layer esta-se a adicionar mais parametros ao modelo. Permitindo que este se ajuste a modelos mais complexos e generalize melhor para dados não vistos.    \n",
    "\n",
    "Para utilizar o backpropagation não é necessário que uma função seja derivavel em todos os pontos, porque se pode utilizar sub-derivadas. Por exmplo a função ReLu tambem não é derivavel para x=0.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/multilayer.jpg\" alt=\"Drawing\" style=\"width: 550px;height: 250px;\"/> </td>    \n",
    "</tr></table>\n",
    "\n",
    "O perceptron de uma camada é um tipo de rede neural com uma camada de entrada, camadas ocultas e uma camada de saida.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron de uma camada para prever o valor da operação xor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron9.png\" alt=\"Drawing\" style=\"width: 650px;height: 200px;\"/> </td>    \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Operador xor:      \n",
    "if(x1 and x2)==False -> class is False    \n",
    "if(x1==False) and (x2==True) -> class is True   \n",
    "if(x1==True) and (x2==False) -> class is True    \n",
    "if(x1 and x2)==True -> class is False    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron10.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>    \n",
    "<td> <img src=\"fotos/perceptron11.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>      \n",
    "</tr></table>\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron12.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>    \n",
    "<td> <img src=\"fotos/perceptron13.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>      \n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculos da função de activação da camada oculta:\n",
    "\n",
    "Atribuindo o valor dos pesos indicado nas figuras.       \n",
    "Usando a função sigmoid como função de activação.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/sigmoid.png\" alt=\"Drawing\" style=\"width: 250px;height: 150px;\"/> </td>  \n",
    "</tr></table>\n",
    "\n",
    "A função sigmoid retorna valores entre 0 e 1. Se x for alto o valor será aproximadamente 1. Se x for pequeno o valor será aproximadamente 0. Não retorna valores negativos.  \n",
    "Sendo x o valor obtido na função soma.  \n",
    "\n",
    " $$função soma =\\sum{xi*wi}$$\n",
    "\n",
    "##### 1º registro:    \n",
    "1º neuronio camada oculta: função soma=0*(-0.424)+0*0.358=0, função de activação(sigmoid)=0.5    \n",
    "2º neuronio camada oculta: função soma=0*(-0.740)+0*(-0.577)=0, função de activação(sigmoid)=0.5  \n",
    "3º neuronio camada oculta: função soma=0*(-0.961)+0*(-0.469)=0, função de activação(sigmoid)=0.5    \n",
    "\n",
    "##### 2º registro:      \n",
    "1º neuronio camada oculta: função soma=0*(-0.424)+1*0.358=0.358, função de activação(sigmoid)=0.589    \n",
    "2º neuronio camada oculta: função soma=0*(-0.740)+1*(-0.577)=-0.577, função activação(sigmoid)=0.360  \n",
    "3º neuronio camada oculta: função soma=0*(-0.961)+1*(-0.469)=-0.496, função activação(sigmoid)=0.385  \n",
    "\n",
    "##### 3º registro:        \n",
    "1º neuronio camada oculta: função soma1*(-0.424)+0*0.358=-0.424, função de activação(sigmoid)=0.396    \n",
    "2º neuronio camada oculta: função soma1*(-0.740)+0*(-0.577)=-0.740, função activação(sigmoid)=0.323  \n",
    "3º neuronio camada oculta: xfunção soma1*(-0.961)+0*(-0.469)=-0.961, função activação(sigmoid)=0.277  \n",
    "\n",
    "##### 4º registro:      \n",
    "1º neuronio camada oculta: função soma=1*(-0.424)+1*0.358=-0.066, função de activação(sigmoid)=0.484    \n",
    "2º neuronio camada oculta: função soma=1*(-0.740)+1*(-0.577)=-1.317, função activação(sigmoid)=0.211  \n",
    "3º neuronio camada oculta: função soma=1*(-0.961)+1*(-0.469)=-1.430, função activação(sigmoid)=0.193  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron14.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>    \n",
    "<td> <img src=\"fotos/perceptron15.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>      \n",
    "</tr></table>\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/perceptron18.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>    \n",
    "<td> <img src=\"fotos/perceptron19.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>      \n",
    "</tr></table>\n",
    "\n",
    "#### Calculos da função de activação da camada de saida:\n",
    "\n",
    "##### 1º registro:    \n",
    "função soma=0.5*(-0.017)+0.5*(-0.893)+ 0.5*0.148=-0.381, função de activação(sigmoid)=0.406  \n",
    "##### 2º registro:    \n",
    "função soma=0.589*(-0.017)+0.360*(-0.893)+0.385*0.148=-0.274, função de activação(sigmoid)=0.432\n",
    "##### 3º registro:    \n",
    "função soma=0.395*(-0.017)+ .323*(-0.893)+0.277*0.148=-0.254, função de activação(sigmoid)=0.437\n",
    "##### 4º registro:    \n",
    "função soma=0.483*(-0.017)+0.211*(-0.893)+0.193*0.148=-0.168, função de activação(sigmoid)=0.458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculo do erro:\n",
    "\n",
    "Para o calculo de erro utilizou-se o algoritmo mais simples em que o erro é:  \n",
    "erro = resposta Correta – resposta Calculada  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/erro.png\" alt=\"Drawing\" style=\"width: 550px;height: 150px;\"/> </td>         \n",
    "</tr></table>\n",
    "A média absoluta do erro é 0.49.  \n",
    "Quanto menor este valor melhor mais ajustados são os valores dos pesos para o problema.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descida do gradiente:\n",
    "O gradiente é calculado para encontrar a combinação de pesos em que o erro é o menor possível. Ou seja encontrar o minimo global da função custo, para saber o valor dos pesos onde o erro é o menor possivel. \n",
    "Para se saber quanto os pesos devem ser ajustados, a direcção a percorrer. Ou seja se o valor do peso deve ser aumentado ou diminuido para se adaptar melhor aos dados. Na figura seguinte está a representação de 2 funções custo.  \n",
    "\n",
    "As redes neurais geralmente executam várias transformações não lineares através da aplicação da função de activação. A função  \n",
    "perda resultante não se parece à figura do lado esquerdo, com apenas um minimo para onde convergir(funções convexas). As funções das redes neurais assemelham-se mais à figura do lado direito com minimos locais e um minimo global.    \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/grad1.png\" alt=\"Drawing\" style=\"width: 350px;height: 200px;\"/> </td>         \n",
    "</tr></table>\n",
    "  \n",
    "A função custo(ou loss function) pode ser vista como uma tigela, e o gradiente dessa função indica a direção de descida mais acentuada, de forma a chegar ao fundo da tigela, onde está o ponto de menor custo(min C(w1, w2 ... wn)).  \n",
    "Para problemas de regressão usa-se normalmente para loss function o Mean Square Error e o Root Mean Square Error.  \n",
    "O MSE como é elevado ao quadrado penaliza mais os erros.\n",
    "\n",
    "Na figura abaixo minimiza-se a função y, usando a descida do gradiente e uma taxa de aprendizagem v=0.1.  \n",
    "Pretende-se encontrar o valor o valor de x que minimiza a função.      \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/func1.png\" alt=\"Drawing\" style=\"width: 250px;height: 800px;\"/> </td>         \n",
    "</tr></table>\n",
    "\n",
    "Nas redes neurais o processo de actualização de pesos é semelhante:  \n",
    "Utilizando a função sigmoid como função de activação e sabendo que o valor da função soma, z, é obtido multiplicando o valor da variavel de entrada pelo respectivo peso e adicionando o bies tem-se a seguinte função sigmoid:      \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/sig.png\" alt=\"Drawing\" style=\"width: 250px;height: 100px;\"/> </td>         \n",
    "</tr></table>\n",
    "\n",
    "Para a função custo, Mean Squared Error, onde N são o numero de amostras(batch_size):\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/cost.png\" alt=\"Drawing\" style=\"width: 150px;height: 50px;\"/> </td>         \n",
    "</tr></table>\n",
    "\n",
    "Pretende-se calcular o gradiente da função custo de modo a minimizar o valor dos pesos e fazer a sua actualização:  \n",
    "Ou seja pretende-se encontrar a direcção da função custo que minimiza os valores dos pesos.    \n",
    "De modo a simplificar os calculos não se calcula directamente a derivada da função custo em relação aos pesos, mas as duas seguintes derivadas.  \n",
    "Sendo n o valor da taxa de aprendizagem.     \n",
    "E actualizando o valor dos pesos pela regra delta (Delta Rule):   \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/cost1.png\" alt=\"Drawing\" style=\"width: 150px;height: 100px;\"/> </td>         \n",
    "</tr></table>\n",
    " \n",
    "A derivada da função custo em relação ao valor previsto:  \n",
    "Considerando N=1.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/cost3.png\" alt=\"Drawing\" style=\"width: 150px;height: 50px;\"/> </td>         \n",
    "</tr></table>   \n",
    "A variavel da função de activação, sigmoid, em relação aos pesos:           \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/cost4.png\" alt=\"Drawing\" style=\"width: 250px;height: 200px;\"/> </td>         \n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Começa-se num ponto onde o erro é alto e pretende-se mover para um ponto onde o erro seja pequeno, por isso tambem se chama de descida do gradiente. Pode-se ver na figura seguinte(representação 2d da função custo) o gradiente a mover-se de um ponto alto(erro alto da primeira iteração) para um ponto mais baixo(erro minimo possivel). Esse percurso é feito de iteração para iteração da rede neural até se atingir o erro pretendido ou o numero de iterações.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/grad2.png\" alt=\"Drawing\" style=\"width: 350px;height: 200px;\"/> </td>         \n",
    "</tr></table>\n",
    " \n",
    "\n",
    "O gradiente (ou vetor gradiente) é um vetor que indica o sentido e a direção na qual, por deslocamento a partir de um ponto obtém-se o maior incremento possível no valor que se pretende calcular.  \n",
    "Para apenas uma dimensão, a noção de derivada e gradiente convergem.  \n",
    "\n",
    "Para uma viga retilínea cujas extremidades estão em contato com duas paredes a temperaturas diferentes, uma à esquerda e outra à direita, sendo a parede à direita a mais quente. Observa-se que a temperatura da viga não é constante em relação ao espaço, e que cresce da esquerda para a direita ao longo de seu comprimento. Definindo a coordenada x=0 para a extremidade à esquerda e x=L para a extremidade à direita, definimos a temperatura T em uma posição qualquer x da viga como uma função T(x).\n",
    "\n",
    "Entre dois pontos muito próximos, distantes de um comprimento dx, tem-se uma respectiva variação de temperatura dT. Em uma situação unidimensional, a razão entre essas grandezas fornece o gradiente:\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gradiente.png\" alt=\"Drawing\" style=\"width: 150px;height: 100px;\"/> </td>         \n",
    "</tr></table>\n",
    "\n",
    "### Backpropagation:\n",
    "A actualização dos pesos é feita utilizando o algoritmo backpropagation.   \n",
    "É um algoritmo utilizado no treino de uma rede neural feedforward. São atribuidos pesos aos neuronios da  rede e os calculos são efectuados com esses pesos partindo da camada de entrada até a camada de saida. Calcula-se o erro obtido na camada de saida e modificam-se os pesos da iteração anterior partindo da camada de saida até a camada de entrada. Utiliza-se o método da descida do gradiente para encontrar o melhor valor para os pesos. De seguida, e com os pesos obtidos na iteração anterior, são efectuados os mesmos calculos.    \n",
    "\n",
    "Chama-se isso de backpropagation porque esta-se a usar o erro da camada de saída para atualizar os pesos, através de iterações, usando a regra da cadeia até que se tenha o valor dos pesos optimos.   \n",
    "O backpropagation é apenas uma aplicação da regra da cadeia (chain rule).    \n",
    "Como as redes neurais são estruturas complicadas, cada peso “contribui” para o erro geral de uma maneira complexa e, portanto, as derivadas reais exigem muito esforço para serem produzidas. No entanto, o backpropagation das redes neurais é equivalente à descida de gradiente típica para regressão logística/linear. Assim, como regra geral de atualizações dos peso, pode-se usar a Regra Delta (Delta Rule):     \n",
    "Novo Peso = Peso Antigo – Derivada * Taxa de Aprendizagem    \n",
    "\n",
    "#### Regra da cadeia(chain rule)\n",
    "A regra da cadeia escrita na notação de Leibniz. Se uma variável z depende da variável y, que por sua vez depende da variável x (isto é, y e z são variáveis dependentes), então z, através da variável intermediária de y, também depende de x. Nesse caso, a regra da cadeia afirma que:  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/1.png\" alt=\"Drawing\" style=\"width: 100px;height: 70px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "A regra da cadeia aplicada às redes neurais:    \n",
    "Nas rede neurais cada neurônio é uma função do anterior conectado a ele. So o valor de w1 for alterado, os neurônios “hidden 1” e “hidden 2” e tambem a saída(output) mudariam. Assim a saida pode ser escrita da seguinte forma:    \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/2.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "Para se aplicar o algoritmo de backpropagation, a função de ativação deve ser diferenciável, de modo que possamos calcular a derivada parcial do erro em relação a um dado peso wi,j, loss(E), saída de nó oj e saída de rede j.\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/3.png\" alt=\"Drawing\" style=\"width: 300px;height: 100px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "A a saída(output) é uma função composta dos pesos, entradas e função (ou funções) de ativação. Os nós ocultos são simplesmente cálculos intermediários que, na realidade, podem ser reduzidos a cálculos da camada de entrada.   \n",
    "Para se calcular a derivada da função de saida em relação a algum peso arbitrário(por exemplo, w1), aplicaria-se iterativamente a regra da cadeia e teria-se:  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/4.png\" alt=\"Drawing\" style=\"width: 400px;height: 100px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "Adicionando o erro da função de saida a rede neural.   \n",
    "O erro é uma função da saída e, portanto é uma função composta de uma função da entrada, pesos e função de ativação.   \n",
    "Para calcular a derivada do erro em relação a qualquer peso arbitrário(w1), usando a regra da cadeia, teria-se:  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/5.png\" alt=\"Drawing\" style=\"width: 500px;height: 150px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "Cada uma dessas derivadas pode ser simplificada, uma vez que se tem uma função de activação e uma função de erro, de modo que todo o resultado represente um valor numérico. Assim a derivada de erro pode ser usada na descida do gradiente para melhorar iterativamente o valor dos pesos.        \n",
    "Calcula-se as derivadas de erro em função dos pesos para todos os outros pesos na rede(w2 e w3) e aplica-se a descida do gradiente. Isso é backpropagation(porque se usa o erro da camada de saída para atualizar os pesos).          \n",
    "\n",
    "\n",
    "### Diferença entre SVM e Redes neurais:\n",
    "\n",
    "Uma rede neural com apenas uma camada oculta(e função de activação não linear) utiliza o mesmo modelo do que o SVM com o kernel trick. Uma rede neural sem camada oculta(utilizada para problemas linearmente separaveis) utiliza o mesmo modelo do que o SVM sem aplicação do kernel trick.   \n",
    "\n",
    "Quando se utiliza uma rede neural, isso significa que se está a tentar encontrar os parâmetros(pesos) que minimizam o erro em relação ao conjunto de dados de treino. O algoritmo usado para actualização dos pesos é quase sempre a descida do gradiente. No SVM tenta-se tambem minimizar o erro em relação ao conjunto de dados de treino e encontrar uma \"hipotese\" que seja simples de modo a que o modelo generalize bem.    \n",
    "Outra diferença reside no facto de a descida do gradiente não encontrar sempre a solução optima para o conjunto de pesos(devido aos minimos locais) enquanto que o SVM encontra sempre esse conjunto de parâmetros que retorna a solução optima.  \n",
    "\n",
    "Para problemas não linearmente separaveis, tanto o SVM como as NN aplicam projecções não lineares para dimensões mais altas de modo a seperar as classes. No caso das NN isto é feito adicionando neuronios nas camadas ocultas. No SVM é utilizado o truque de kernel.   \n",
    "O truque de kernel não aumenta a complexidade computacional com o aumento de dimensões, enquanto que nas NN aumenta com o aumento do numero de neuronios.    \n",
    "O processo de treino dos SVM é mais rapido do que nas NN. As previsões são mais lentas para o SVM se forem utilizados muitos vectores de suporte(muitas dimensões).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actualização dos pesos do exemplo \n",
    "DeltaSaida=Erro * DerivadaSigmoid  \n",
    "Onde a derivada da Sigmoid utilizada é a da camada de saida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta1.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>    \n",
    "<td> <img src=\"fotos/delta2.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>      \n",
    "</tr></table>\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta3.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>    \n",
    "<td> <img src=\"fotos/delta13.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>      \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeltaEscondida=DerivadaSigmoid * peso * DeltaSaida    \n",
    "Onde a derivada da Sigmoid utilizada é a da camada oculta e o peso utilizado é o que faz a ligação entre a camada oculta e camada de saida.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta5.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>    \n",
    "<td> <img src=\"fotos/delta16.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>      \n",
    "</tr></table>\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta9.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>    \n",
    "<td> <img src=\"fotos/delta8.png\" alt=\"Drawing\" style=\"width: 450px;height: 250px;\"/> </td>      \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da actualização dos pesos utilizando a seguinte formula:  \n",
    "peso(n+1) = (peso(n) * momento)+(entrada * delta * taxa de aprendizagem)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da entrada(Valor do neurónio da camada oculta)*delta(delta da camada de saida) para o 1º neurónio da camada oculta.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta10.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px;\"/> </td>        \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da entrada(Valor do neurónio da camada oculta)*delta(delta da camada de saida) para o 2º neurónio da camada oculta.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta11.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px;\"/> </td>        \n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da entrada(Valor do neurónio da camada oculta)*delta(delta da camada de saida) para o 3º neurónio da camada oculta. \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta12.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px;\"/> </td>        \n",
    "</tr></table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualização dos pesos da camada oculta para a camada de saida:\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta14.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px;\"/> </td>        \n",
    "</tr></table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da entrada(Valor do neurónio da camada entrada)*delta(delta da camada oculta) para o 1º neurónio da camada oculta. \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta17.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px;\"/> </td>        \n",
    "</tr></table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da entrada(Valor do neurónio da camada entrada)*delta(delta da camada oculta) para o 2º neurónio da camada oculta.\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta18.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px;\"/> </td>        \n",
    "</tr></table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo da entrada(Valor do neurónio da camada entrada)*delta(delta da camada oculta) para o 3º neurónio da camada oculta.\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta19.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px;\"/> </td>        \n",
    "</tr></table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualização dos pesos da camada de entrada para a camada oculta:\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/delta20.png\" alt=\"Drawing\" style=\"width: 650px;height: 450px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "Após esta primeira actualização dos pesos, volta-se a repetir o processo. A rede passa os dados da camada de entrada para a camada oculta. Calcula-se a função soma e aplica-se a função de activação. De seguida a rede passa os valores da camada oculta para a camada de saida, calcula a função soma e aplica a função de activação. Calcula o erro obtido e ajusta os pesos por backpropagation. Este processo é repetido por várias épocas ou até se atingir um erro definido.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "Colocação de um neurónio adicional(normalmente com o valor 1) na camada de entrada e nas camadas ocultas.  \n",
    "Isto permite obter valores diferentes de zero na função de soma mesmo que todos os valores da camada de entrada sejam zero.  \n",
    "Permite que a rede se ajuste melhor aos dados e seja mais flexivel.  \n",
    "\n",
    "O papel da bias é semelhante à constante b de uma função linear(y = ax + b).  \n",
    "Este parâmetro b permite mover a linha para cima e para baixo para se ajustar melhor aos dados. Sem ele a linha passava sempre pela origem(0,0) e poderia dar um mau ajuste do modelo aos dados.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bies.png\" alt=\"Drawing\" style=\"width: 450px;height: 350px;\"/> </td>        \n",
    "</tr></table>  \n",
    "\n",
    "Considerando a seguinte rede com input(x) e peso (wo) e função de activação sigmoid.  \n",
    "Para vários valores de w0 tem-se o seguinte gráfico:  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bias1.png\" alt=\"Drawing\" style=\"width: 450px;height: 450px;\"/> </td>        \n",
    "</tr></table>  \n",
    "\n",
    "Alterando o valor do peso(w0) altera apenas o \"degrau\" da função de activação sigmoid. Caso se queira obter o resultado(output)\n",
    "0 quando x é 2 alterar apenas os valores dos pesos não chega. Tem-se de conseguir mover a curva da função para a direita.\n",
    "Com a bias isso é possivel.  \n",
    "\n",
    "Caso se adicione bias à rede fica-se com a seguinte configuração:  \n",
    "Onde o resultado(output) retorna o resultado da função sigmoid aplicado à função soma(w0*x + w1*1.0).  \n",
    "Na figura pode-se ver o resultado para vários valores de w1.  \n",
    "Com um peso de -5 para w1 a curva move-se para a direita e permite que a rede retorne um resultado de 0 para um x de 2.    \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bias2.png\" alt=\"Drawing\" style=\"width: 450px;height: 550px;\"/> </td>        \n",
    "</tr></table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de descidas de gradiente\n",
    "\n",
    "batch_size:\n",
    "\n",
    "- Se escolher-se 1(Stochastic gradient descent) calcula-se o erro para cada dado e atualiza-se os pesos de seguida.\n",
    "- Se escolher-se igual ao numero de dados (batch gradient descent) calcula-se o erro para todos os registros e só depois é que se actualizam os pesos.\n",
    "- Se escolher-se um número intermédio(Mini batch gradient descent), por ex. 10, calcula-se o erro para um conjunto de 10 dados e actualizam-se os pesos (Stochastic gradient descent).\n",
    "\n",
    "epochs:  Quantas vezes o algoritmo percorre o conjunto de dados de treino totalmente.\n",
    "\n",
    "Caso a base de dados de treino tenha 100 dados.\n",
    "\n",
    "#### Stochastic gradient descent:\n",
    "\n",
    "1ª época: Calcula o erro para o 1º registro e actualiza todos os pesos. Calcula o erro para o 2º registro e actualiza todos os pesos.  Calcula o erro para o 3º registro e actualiza todos os pesos.  E assim sucessivamente até aos 100 registros.\n",
    "\n",
    "2ª época: Calcula o erro para o 1º registro e actualiza todos os pesos da 1ª época. Calcula o erro para o 2º registro e actualiza todos os pesos da iteração anterior.  Calcula o erro para o 3º registro e actualiza todos os pesos da iteração anterior.  E assim sucessivamente.\n",
    "\n",
    "##### Vantagens:\n",
    "- Requer pouca memoria porque apenas um registro é processado de cada vez. \n",
    "- É computacionamente rapido porque apenas um registro e processado de cada vez.  \n",
    "- Para grandes conjuntos de dados pode convergir rápido porque actualiza os parâmetros mais frequentemente.  \n",
    "- Devido às frequentes actualizações os passos dados em direcção aos minimos locais da função custo têm oscilações o que ajuda a sair destes minimos.\n",
    "\n",
    "##### Desvantagens: \n",
    "- Devido às actualizações frequentes os passos dados em direcção ao minimo global têm muito \"ruido\". O que pode levar o gradiente para outras direcções.  \n",
    "- Devido ao \"ruido\" pode levar mais tempo para se obter a convergencia para o minimo global da função custo.  \n",
    "- As actualizações frequentes são computacionalmente caras porque se estão a utilizar todos os recursos para processar apenas um registro.  \n",
    "\n",
    "#### batch gradient descent:\n",
    "\n",
    "1ª época: Calcula o erro para o 1º registo. Calcula o erro para o seguindo registo. E assim sucessivamente até ter calculado\n",
    "o erro para os 100 registro. No fim actualiza todos os pesos.\n",
    "\n",
    "2ª época: Calcula o erro para o 1º registo. Calcula o erro para o seguindo registo. E assim sucessivamente até ter calculado\n",
    "o erro para todos os registro. No fim actualiza todos os pesos da 1ª época.\n",
    "\n",
    "##### Vantagens:\n",
    "- Os passos do gradiente em direcção ao minimo global têm menos ruido e oscilação porque a actualização é menos frequente.\n",
    "- A convergencia e o erro em relação ao minimo são mais estáveis do que o stochastic gradient descent.\n",
    "- É computacionalmente mais eficiente porque os recursos estão a ser usados para processar todas as amstras de treino.\n",
    "\n",
    "##### Desvantagens:  \n",
    "- Às vezes um erro do gradiente estável pode conduzir o gradiente para um minimo local e ao contrário do stochastic gradient descent não existem passos com ruido para ajudar a sair desse minimo local.  \n",
    "- O conjunto de dados de treino pode ser demasiado grande para ser processado na memoria do computador.  \n",
    "- Pode demorar demasiado tempo para processar o conjunto de dados de treino todo como uma amostra(batch).    \n",
    "\n",
    "#### Mini batch gradient descent (por ex. batch=10):\n",
    "\n",
    "1ª época: Calcula o erro para os 1ºs 10 registros e depois actualiza todos os pesos. Calcula o erro para os 2ºs 10 registros e actualiza todos os pesos.\n",
    "\n",
    "2ª época: Calcula o erro para os 1ºs 10 registros e depois actualiza todos os pesos da 1ª época. Calcula o erro para os 2ºs 10 registros e actualiza todos os pesos da iteração anterior.\n",
    "\n",
    "##### Vantagens:\n",
    "- Procura encontrar um balanço entre a rebustez do stochastic gradient descent e a eficiência do batch gradient descent.\n",
    "- A actualização dos pesos é mais frequente do que no batch gradient descent o que torna a convergencia mais rebusta, evitando minimos locais.  \n",
    "- Utilizar conjuntos de dados(batch) é computacionalmente mais eficiente do que utilizar o stochastic gradient descent.  \n",
    "- Utilizar conjuntos de dados(batch) permite que não se tenha todo o conjunto de dados de treino em memoria.  \n",
    "- Se ficar preso em minimos locais, alguns passos com ruido ajudam a sair desses minimos locais.  \n",
    "- Utilizar um conjunto de amostrar(batch) faz com que a convergencia e erro em relação ao gradiente sejam estáveis.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de activação\n",
    "\n",
    "As funções de ativação introduzem um componente não linear nas redes neurais, que faz com que elas possam aprender mais do \n",
    "que relações lineares entre as variáveis dependentes e independentes. Excepto a setep function que é apenas utilizada para problemas linearmente separáveis.  \n",
    "\n",
    "#### Step Function\n",
    "Retorna o valor 0 ou 1.  \n",
    "Define-se um valor limite. Se for maior do que esse valor, o resultado é 1. Se for for menor ou igual o resultado é 0. \n",
    "Utilizada para problemas linearmente separáveis.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/step.png\" alt=\"Drawing\" style=\"width: 250px;height: 250px;\"/> </td>        \n",
    "</tr></table>  \n",
    "\n",
    "#### Função sigmoid\n",
    "Retorna valores entre 0 e 1(Uma probabilidade).    \n",
    "Função não linear.  \n",
    "Utilizada na camada de saida para problemas de classificação binária.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/sigmoid1.png\" alt=\"Drawing\" style=\"width: 250px;height: 250px;\"/> </td>        \n",
    "</tr></table>  \n",
    "\n",
    "#### Função tangente hiperbólica\n",
    "Retorna valores entre -1 e 1.  \n",
    "Função não linear.  \n",
    "Utilizada na camada de saida para problemas de classificação binária.  \n",
    "Se for utilizada nas camadas ocultas e caso se tenha valores de entrada negativos esta função modela esses valores.   \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/tangente.png\" alt=\"Drawing\" style=\"width: 250px;height: 250px;\"/> </td>        \n",
    "</tr></table>  \n",
    "\n",
    "#### Função ReLU (rectified linear units)\n",
    "Retorna valores maiores ou iguais a 0.  \n",
    "Função não linear(o seu resultado não é uma linha recta). \n",
    "Usada nas camadas ocultas.  \n",
    "Quando se tem entradas com muitos valores negativos o modelo não irá funcionar bem. Enquanto que a função tangente hiperbolica mapeia os valores muito negativos como -1 e os valores muito possitivos como 1, a função relu mapeia os valores negativos como 0 e os possitivos como o seu valor real.       \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/relu.png\" alt=\"Drawing\" style=\"width: 250px;height: 150px;\"/> </td>        \n",
    "</tr></table>  \n",
    "\n",
    "#### Função Softmax\n",
    "Retorna probabilidades para as várias classes. Por ex. para um neurónio tem-se a probabilidade de 10% de ser iris setosa, noutro neurónio 80% iris versicolor e noutro 10% de ser iris verginica. A classe atribuida será aquela que tem maior probabilidade, neste ex. a iris verginica.          \n",
    "Função não linear.       \n",
    "Utilizada na camada de saida para problemas de classificação multi-classe.   \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/softmax.png\" alt=\"Drawing\" style=\"width: 450px;height: 350px;\"/> </td>        \n",
    "</tr></table>  \n",
    "\n",
    "#### Função Linear\n",
    "Retorna o valor de entrada.    \n",
    "Função linear.    \n",
    "Utilizada na camada de saida para problemas de regressão. \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/linear.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>        \n",
    "</tr></table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais Convolucionais <a name=\"convolucionais\"></a>\n",
    "  \n",
    "São redes neurais usadas exclusivamente para visão computacional(processamento de videos e imagens).  \n",
    "Como a construção de carros autonomos, detecçao de peões, detecção de objectos e faces, etc.  \n",
    "Normalmente funciona melhor do que o algoritmo SVM (support vector machines) que é considerado o algoritmo mais eficiente de Machine Learning.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixeis\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis.png\" alt=\"Drawing\" style=\"width: 350px;height: 350px;\"/> </td>        \n",
    "</tr></table> \n",
    " \n",
    " \n",
    "Cada pixel é formado pelos valores RGB(Red, Green, Blue). A combinação dessas 3 cores vai formar a cor do pixel. Uma imagem a cores tem 3 canais dentro de cada pixel(como se o pixel fosse dividido em 3).  \n",
    "Numa imagem a preto e branco(escala de cinzento) cada pixel tem apenas 1 canal. Com rgb=1 tem-se menos dados para processar.  \n",
    "\n",
    "Cada pixel ocupa 1 byte e o byte consegue guardar 256 resultados possiveis(ou seja o pixel varia entre 0 e 255).  \n",
    "Na escala de cinzento(rgb=1) quanto mais próximo de 255 mais claro é o cinzento e quanto mais próximo de 0 mais escura é a cor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base de dados\n",
    "\n",
    "Pode-se construir uma base de dados para classificação de imagens extraindo caracteristicas das imagens.  \n",
    "Por exemplo o formato do cabelo é diferente no Bart em relação ao Homer. Os olhos são iguais então não pode ser usado como caracteristica. O tamanho da cintura pode ser utilizado.  \n",
    "\n",
    "Os vectores seguintes têm as seguintes caracteristicas:   \n",
    "Camisa cor de laranja  \n",
    "Calções azuis   \n",
    "Sapatos azuis  \n",
    "Boca castanha  \n",
    "Calças azuis  \n",
    "Sapato cinzento  \n",
    "Personagem(Bart ou Homer)  \n",
    "\n",
    "Nos primeiros 3 vectores tem-se 3 caracteristicas ezpecificas do Bart com valores e nos ultimos 3 vectores tem-se 3 caracteristicas especificas do Homer com valores.  \n",
    "Por exemplo no primeiro vector pode-se ver que a personagem é o Bart e por isso as caracteristicas associadas ao Homer(cor da boca, cor dos sapatos e cor das calças) são todas 0.\n",
    "Num projecto mais completo tinha-se de usar mais caracteristicas para alem das cores.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis1.png\" alt=\"Drawing\" style=\"width: 650px;height: 350px;\"/> </td>        \n",
    "</tr></table>   \n",
    "\n",
    "Para classificação de caracteres escritos à mão, onde o objectivo é passar uma imagem e prever se essa imagem é o caracter 0,1,2,etc.    \n",
    "Essas imagens podem ser representadas pelo valor de cada pixel.  Na figura seguinte tem-se a imgem constituida por 5x5 pixeis a preto e branco desconstruida num vector de zeros(pixeis brancos) e uns(pixeis pretos).  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis3.png\" alt=\"Drawing\" style=\"width: 650px;height: 350px;\"/> </td>        \n",
    "</tr></table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes neurais artificiais vs convolucionais\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis4.png\" alt=\"Drawing\" style=\"width: 650px;height: 350px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "#### Redes neurais artificiais\n",
    "Para as imagens do homer no exemplo acima, extraiu-se as caracteristicas de cada uma das personagens. Para isso é verificado manualmente as caracteristicas que são especificas de cada uma das personagens e desenvolve-se um extractor de caracteristicas que percorre as imagens pixel a pixel e faz a contagem da percentagem da cor no pixel(laranja na camisa, azul no sapato, etc.) e constroi-se a base de dados que depois é submetida para a rede neural artificial ou um algoritmo de machine learning.  \n",
    "Tem o problema de ter-se de identificar as caracteristicas mais importantes para cada imagem, o que por vezes pode ser dificil. \n",
    "Para reconhecimento facial mormalmente utiliza-se a localização do nariz, distancia entre os olhos e a localização da boca.\n",
    "\n",
    "Para o exemplo dos caracteres escritos à mão fez-se a leitura pixel a pixel e atribui-se um valor a cada um dos pixeis(valor 1 quando o pixel tem informação e 0 quando não tem). Os vectores formados pelo conjunto dos pixeis são passados para a a rede neural artificial ou um algoritmo de machine learning.\n",
    "Neste caso tem-se apenas imagens 5x5=25 pixeis. Se a imagem for a de uma fotografia com uma resolução de 2000x1500 pixeis fica inviavel de se utilizar uma rede neural artificial com tantos dados de entrada.  \n",
    "\n",
    "\n",
    "#### Redes neurais convolucionais\n",
    "As redes neurais convolucionais são utilizadas para solucionar o problema da extração de caracteristicas e o problema do tratamento de imagens com muitos pixeis.    \n",
    "Não utiliza todas as entradas(pixeis). Descobre as caracteristicas mais importantes. Na figura acima utiliza por exemplo o formato da boca, da sobrancelha e do olho e não utiliza a testa. Não se tem o problema de ter-se demasiadas entradas.   \n",
    "Utiliza uma rede neural densa, mas no inicio transforma os dados na camada de entrada.    \n",
    "Descobre quais são as caracteristicas mais importantes. Não é necessário fazer a extração de caracteristicas manualmente. Por exemplo no primeiro imoji da figura acima utiliza apenas o sorriso, no seguindo utiliza apenas os olhos, no terceiro as sobrancelhas, a boca e a lagrima.  \n",
    "Se fosse utilizada uma rede neural artificial teria-se de usar todos os pixeis. Com uma rede neural convoluconal faz-se em primeiro um pré-processamento das imagens para descobrir as caracteristicas mais importantes e depois usa-se uma rede neural densa.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etapas das redes neurais convolucionais\n",
    "As redes neurais convolucionais têm as seguintes etapas: \n",
    "- Operador de convolução  \n",
    "- Pooling  \n",
    "- Flattening  \n",
    "- Rede neural densa  \n",
    "\n",
    "Sendo as 3 primeiras etapas de pré-processamento das imagens para descobrir as caracteristicas mais importantes.  \n",
    "\n",
    "#### Operador de convolução  \n",
    "\n",
    "A convolução é o processo de adicionar cada elemento da imagem para seus vizinhos, ponderado por um kernel.  \n",
    "A imagem é uma matriz e o kernel(ou detector de caracteristicas ou filtro) é outra matriz.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis10.png\" alt=\"Drawing\" style=\"width: 250px;height: 150px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "##### Mapa de caracteristicas:\n",
    "Considerando a imagem 7x7 obtem-se o mapa de caracteristicas da imagem multiplicando essa imagem pelo detector de caracteristicas 3x3.  \n",
    "O tamanho do detector de caracteristicas deve ser alterado quado se tem imagens maiores.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis5.png\" alt=\"Drawing\" style=\"width: 650px;height: 250px;\"/> </td>\n",
    "<td> <img src=\"fotos/pixeis6.png\" alt=\"Drawing\" style=\"width: 650px;height: 250px;\"/> </td>      \n",
    "</tr></table> \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis7.png\" alt=\"Drawing\" style=\"width: 650px;height: 250px;\"/> </td>\n",
    "<td> <img src=\"fotos/pixeis8.png\" alt=\"Drawing\" style=\"width: 650px;height: 250px;\"/> </td>      \n",
    "</tr></table> \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis9.png\" alt=\"Drawing\" style=\"width: 650px;height: 250px;\"/> </td>    \n",
    "</tr></table> \n",
    "\n",
    "- Com o mapa de características (filter map) a imagem fica menor o que facilita o processamento.  \n",
    "- O objectivo é detectar as partes principais da imagem (quanto maior os números do mapa melhor, ou seja, mais importante é a caracteristica).\n",
    "- Alguma informação da imagem pode ser perdida.   \n",
    "- O mapa de características preserva as características principais da imagem (olho, boca, nariz, por exemplo). \n",
    "\n",
    "A função ReLu é aplicada ao mapa de caracteristicas(a cada um dos valores da matriz) para transformar os valores negativos em zero. Isto permite detectar melhor os padrões.   \n",
    "Na imagem abaixo pode-se ver que com a aplicação da função relu tranforma-se as partes mais escuras da imagem em mais claras, semelhantes ao resto da imagem.    \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis11.png\" alt=\"Drawing\" style=\"width: 700px;height: 250px;\"/> </td>    \n",
    "</tr></table> \n",
    "\n",
    "##### Camada de convolução\n",
    "A camada de convolução é o conjunto dos mapas de caracteristicas. Na construção de uma rede neural utiliza-se mais do que um mapa de caracteristicas e depois a rede decide qual é o detector de características melhor.  \n",
    "\n",
    "A rede neural testa vários detectores com valores diferentes para descobrir o melhor. Durante o treino do modelo além de se descobrir os melhores pesos para o modelo tambem se descobre qual é que é o melhor detector de caracteristicas.  \n",
    "\n",
    "Existem vários tipos de kernel pré-definidos. Para se manter a identidade da imagem, para detecção das bordas das figuras da imagem, para se obter um efeito de nevoeiro, etc.  \n",
    "No TensorFlow esse detector de caracteristicas é inicializado aleatoriamente e é actualizado no fim de cada época alterando os seus valores da matriz.   \n",
    "\n",
    "Cada detector de caracteristicas origina um mapa de caracteristicas diferente.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis12.png\" alt=\"Drawing\" style=\"width: 700px;height: 250px;\"/> </td>    \n",
    "</tr></table> \n",
    "\n",
    "#### Pooling\n",
    "A etapa de pooling serve para se enfatizar ainda mais as caracteristicas dos objectos nas imagens.  \n",
    "Seleciona as características mais relevantes (reduz overfitting e ruídos desnecessários).  \n",
    "Caso se utilize o Max polling, está-se a focar nas características mais relevantes. Tambem se pode utilizar o minimo e a média. \n",
    "No caso das redes neurais utiliza-se o Max polling para se obter as caracteristicas mais relevantes.  \n",
    "\n",
    "Nas figuras seguintes utilizou-se o Max polling para se obter a matriz de Max pooling.    \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis13.png\" alt=\"Drawing\" style=\"width: 800px;height: 800px;\"/> </td>    \n",
    "</tr></table> \n",
    "\n",
    "##### Camada de pooling\n",
    "A camada de pooling é composta pelas matrizes de pooling obtidas utilizando os diferentes mapas de caracteristicas.  \n",
    "Cada mapa de caracteristicas produz uma matriz de pooling diferente.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis14.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>    \n",
    "</tr></table> \n",
    "\n",
    "#### Flattening\n",
    "Transformar as matrizes de pooling em vectores para ser compativel com o formato da camada de entrada da rede neural densa.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis15.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>    \n",
    "</tr></table> \n",
    "\n",
    "#### Rede neural densa  \n",
    "\n",
    "Tem-se a classe 1,3 e classe 9.    \n",
    "Quando os valores dos 2 primeiros neurónios da ultima camada oculta são altos a classe correnpondente é a classe 1, ou seja é activado o 1º neurónio da camada de saida.   \n",
    "Por exemplo a rede neural recebe 1000 imagens com o numero 1 e na maioria das vezes activa os 2 primeiros neurónios da ultima camada oculta com valores altos. A rede neural aprende que quando os 2 neurónios da ultima camada têm valores altos trata-se de uma imagem da classe 1. A rede neural aprende quais são os valores que têm de chegar à ultima camada oculta que esteja relacionado com a previsão.     \n",
    "\n",
    "Quando os valores são altos nos 2 ultimos neurónios da ultima camada oculta trata-se de uma imagem da classe 3. Os 2 ultimos neurónios da ultima camada oculta são activados quando se trata de uma imagem da classe 3. Durante o treino a rede neural aprende que quando os 2 ultimos neurónios da ultima camada oculta têm valores altos trata-se de uma imagem da classe 3.\n",
    "\n",
    "Quando o valor do 3º neurónio da ultima camada oculta trata-se de uma imagem da classe 9. O 3º neurónio da ultima camada oculta é activado quando se trata de uma imagem da classe 9. Durante o treino a rede neural aprende que quando o 3º neurónio da ultima camada oculta têm valores altos trata-se de uma imagem da classe 9.\n",
    "\n",
    "Na ultima imagems tem-se valores altos nos 2 primeiros neurónios(0.9 e 0.6) e tambem se tem valores altos no 3º neurónio(0.6). Existe a probabilidade de ser 65% da classe 1, 30% de ser da classe 9 e 5% da classe 3.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis16.png\" alt=\"Drawing\" style=\"width: 400px;height: 300px;\"/> </td>    \n",
    "<td> <img src=\"fotos/pixeis17.png\" alt=\"Drawing\" style=\"width: 400px;height: 300px;\"/> </td>      \n",
    "</tr></table> \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis18.png\" alt=\"Drawing\" style=\"width: 400px;height: 300px;\"/> </td>    \n",
    "<td> <img src=\"fotos/pixeis19.png\" alt=\"Drawing\" style=\"width: 400px;height: 300px;\"/> </td>      \n",
    "</tr></table> \n",
    "\n",
    "#### Estrutura das redes neurais convolucionais\n",
    "O processo de treino das redes neurais convolucionais é feito com a descida do gradiente. Para além do ajuste dos pesos, é feito também a mudança do detector de características no fim de cada época.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/pixeis20.png\" alt=\"Drawing\" style=\"width: 700px;height: 300px;\"/> </td>        \n",
    "</tr></table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais Recorrentes <a name=\"recorrentes\"></a>\n",
    "As redes neurais recorrentes são redes neurais usadas quando se tem dados sequenciais(dados com intervalos de tempo) para prever o que vai acontecer a seguir.    \n",
    "Pode ser utilizada para prever acções, por exemplo em videos,com os frames anteriores é possivel prever o proximo frame.  \n",
    "Para processamento de linguagem natural, para previsão da próxima palavra em um texto, tradução automática e geração de poemas. \n",
    "Para geração de legendas.  \n",
    "Para séries temporais (time series), como o preço de ações na bolsa de valores, a Temperatura, crescimento populacional e nível de poluição.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn.png\" alt=\"Drawing\" style=\"width: 400px;height: 200px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "Por exemplo para se entender o final de uma frase é necessário saber o que foi dito antes, ou para se prever o vai acontecer no dia x+1 é necessário saber o que aconteceu no dia x e x-1.   \n",
    "As redes neurais tradicionais não armazenam informações no tempo(fazem previsões independentes). As informações só passam de camada para camada(passam da camada de entrada para a camada oculta e da camada oculta para a camada de saida) e não armazenam a informação.       \n",
    "Já as redes neurais recorrentes usam loops(ou repetições) que permitem que a informação persista.\n",
    "Estas redes neurais são múltiplas cópias de si mesmas devido aos loops. Por exemplo se uma camada oculta tiver 50 loops isso significa que cada neurónio envia a informação para si mesmo e para a frente 50 vezes. Ou seja são 50 cópias dessa camada.     \n",
    "As redes neurais recorrentes têm um loop nas camadas ocultas que envia a informação para o mesmo neurónio e tambem para a frente, em vez de apenas aplicar a função de activação e passar a informação para a frente.    \n",
    "\n",
    "Na proxima figura o neurónio A de uma camada oculta recebe xt e envia essa informação para si mesmo e para a frente para ht. Como essa informação é enviada para si mesmo entra em loop. Ou seja o A(tempo 0) passa a informação para o A(tempo 1) que volta a passar a informação para o A(tempo 2) e de todas estas vezes a informação tambem é passada para a frente. Isto é feito até a um numero de loops(repetições ou numero de células de memória) definidos.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn1.png\" alt=\"Drawing\" style=\"width: 400px;height: 400px;\"/> </td>    \n",
    "<td> <img src=\"fotos/rnn3.png\" alt=\"Drawing\" style=\"width: 200px;height: 400px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "Outra forma de representação das redes neurais recorrentes é espremer as camadas e cada camada da RNN será somente um único nó, mas cada nó representa uma camada que pode conter vários neurônios(3ª figura). A camada escondida é representada\n",
    "por somente um único neurônio que representa 4 neurônios. A camada escondida é conectada a ela mesma, mandando uma resposta para frente e também se realimentando(2ª figura).  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn2.png\" alt=\"Drawing\" style=\"width: 400px;height: 400px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "O treino das redes neurais recorrentes é feito utilizando Backpropagation, desenrolando a rede neural recorrente em uma rede neural padrão.    \n",
    "Os pesos são compartilhados, a memória é rápida, curta e lembra o que aconteceu apenas nas últimas interações.    \n",
    "A auto ligação é o temporal no loop. Medir o quanto um dado está longe um do outro pode ser feito por essa auto ligação.  \n",
    "\n",
    "### Gradiente desaparecendo (vanish gradient)\n",
    "O ideal é que o gradiente seja maior nas primeiras iterações de modo a que a rede aprenda mais rapidamente, porque é pouco provavel que o minimo da função custo seja encontrado logo nessas iterações, e que vá diminuindo nas iterações seguintes, quando se aproxima do minimo global da função.    \n",
    "\n",
    "Na proxima figura tem-se a representação de um neurónio da camada oculta com um tempo igual a 6, ou seja vai armazenar as 6 informações anteriores.\n",
    "Em primeiro faz-se os calculos da esquerda para a direita(feedforward) e depois da direita para a esquerda(backpropagation) para actualização dos pesos utilizando backpropagation e a descida do gradiente. \n",
    "À medida que se adiçona mais camadas à rede neural e usando certas funções de activação(tanh, sigmoid), o gradiente da função erro aproxima-se de zero dificultando o treino da rede neural.\n",
    "\n",
    "Algumas funções de ativação, como a função sigmóide, comprimem o valor da função soma num valor entre 0 e 1. Portanto, uma grande mudança no valor da função soma causará uma pequena alteração na função de activação. Assim, a derivada se torna pequena.  \n",
    "Por exemplo, na figura abaixo está a função sigmóide e sua derivada. Quando as entradas da função sigmoide se tornam maiores ou menores (quando x se torna maior), a derivada torna-se próxima de zero.\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/sig4.png\" alt=\"Drawing\" style=\"width: 400px;height: 200px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "Para redes neurais com poucas camadas e que usam estas funções de activação(sigmoid e tanh) isto não é um grande problema. No entanto, com a adição de mais camadas, pode fazer com que o gradiente seja demasiado pequeno para que o treino funcione de maneira eficaz.  \n",
    "\n",
    "Os gradientes das redes neurais são encontrados usando o algoritmo de backpropagation. Este algoritmo encontra as derivadas da rede, movendo camada por camada da camada final para a inicial. Pela regra da cadeia, as derivadas de cada camada são multiplicadas pela rede(da camada final até a inicial) para calcular as derivadas das camadas iniciais.  \n",
    "\n",
    "No entanto, quando n camadas ocultas usam uma função de activação como a função sigmoide e tanh, n pequenas derivadas são multiplicadas juntas. Assim, o gradiente diminui exponencialmente à medida que é nos propagamos até as camadas iniciais.\n",
    "\n",
    "Um gradiente pequeno significa que os pesos e vieses das camadas iniciais não serão atualizados com eficácia. Como essas camadas iniciais costumam ser fundamentais para reconhecer os elementos principais dos dados de entrada, isso pode levar a que o modelo obtido não seja um bom modelo.    \n",
    "  \n",
    "Assim as contribuições do gradiente de etapas mais “longínquas” tornam-se zero e o estado nessas etapas não contribui para a aprendizagem da rede. A rede acaba não aprendendo dependências de longo alcance.    \n",
    "\n",
    "O problema da descida do gradiente não é exclusivo das RNNs e também acontecem em Redes Neurais Profundas Feedforward. Mas as RNNs tendem a ser muito profundas, o que torna o problema muito mais comum.    \n",
    "\n",
    "### Exploding gradient\n",
    "Os gradientes provenientes da camada de saida da rede têm que passar por multiplicações contínuas de matrizes por causa da regra da cadeia, e quando se aproximam da camada de entrada, se tiverem valores pequenos (<1), eles diminuem exponencialmente até desaparecerem e tornar impossível o modelo de aprender, esse é o problema do gradiente desaparecendo.       \n",
    "Enquanto, por outro lado, se os gradientes tiveres valores grandes (> 1) eles ficam maiores e eventualmente explodem.       \n",
    "\n",
    "Dependendo das funções de ativação utilizadas(a ReLu não sofre desse problema) e parâmetros da rede(learning rate), pode-se obter explosão do gradiente em vez de desaparecimento do gradiente.      \n",
    "O valor do gradiente poderá ficar muito grande. Se isso acontecer a rede neural tem dificuldade em encontrar o minimo global da função custo, para saber o valor dos pesos onde o erro é o menor possivel.    \n",
    "\n",
    "A razão pela qual o problema do gradiente desaparecendo é mais problemático é porque as explosões de gradientes são óbvias. Os Seus gradientes tornam-se NaN (não é um número) e seu rede neural dá um erro. É possivel definir um valor máximo para os gradientes, o que é uma solução simples e eficaz para evitar a explosão dos gradientes.       \n",
    "O problema do desaparecimento do gradiente é mais problemático porque não é tão obvio e quando ocorrem é mais complicado lidar com ele.        \n",
    "\n",
    "### Soluções para o problema do gradiente desaparecendo\n",
    "- Utilizar Backpropagation through time (BPTT): algoritmo semelhante que também fluirá para trás a partir do tempo futuro para os tempos atuais.  \n",
    "- Utilizar a inicialização dos pesos Xavier initialization(glorot_normal no keras). Manter o valor da variancia constante de camada para camada. Isto ajuda a que não se tenha valores demasiado altos ou demasiados baixos. Ou seja inicializar os pesos de modo a que a variancia continue a mesma para x e y. Uma maneira de conseguir isso é atribuir aos valores dos pesos uma distruibuição gaussiana. Esta distribuição de pesos teria média 0 e variancia finita.       \n",
    "- Utilizar redes neurais recorrentes LSTM (long short term memory).      \n",
    "- A solução mais simples é utilizar outras funções de activação como a ReLU que não causam pequenas derivadas(A derivada ReLU é uma constante de 0 ou 1).    \n",
    "\n",
    "### Soluções para o problema do gradiente explodindo\n",
    "- Não utilizar todas as camadas ocultas para o treino. Como cada neurónio envia a informação para si mesmo tornando a rede neural uma cópia de si mesma pode-se não utilizar todas as camadas ocultas para fazer o treino da rede, apesar de não ser recomendado.  \n",
    "- Utilizar o optmizador RMSProp: divide a taxa de aprendizagem por uma média exponencialmente decrescente do quadrado do gradiente.    \n",
    "- Clipping gradiente (grampo). Definir um valor máximo para o gradiente.    \n",
    "\n",
    "## Redes neurais recorrentes LSTM (long short term memory)\n",
    "Estas redes neurais foram projetadas para lidar com o problema da descida do gradiente e aprender eficientemente dependências de longo alcance.    \n",
    "São redes neurais que aprendem padrões. Para redes neurais recorrente simples é mais dificil.      \n",
    "Adiciona células de memória na rede neural e manipula essas células de modo a manter apenas a informação relevante.              Nas redes neurais recorrentes simples apenas se tem o nó temporal(o neuronio aponta para ele mesmo), nestas redes tem-se uma parte adicional, as células de memória.         \n",
    "Aprende dependências de longo prazo(através dessa manipulação das células de memória).    \n",
    "\n",
    "\n",
    "Por exemplo dada a seguinte frase \"“As nuvens estão no .” quer-se obter a seguinte frase \"As nuvens estão no ceu\".  \n",
    "Como o espaço entre as palavras é pequeno, pode-se usar a informação passada(\"As nuvens estão no\") e por isso pode ser utilizada uma rede neural recorrente simples(de memória curta).    \n",
    "\n",
    "Para a seguinte frase \"Eu sou do Brasil...\" quer-se obter o seguinte \"Eu sou do Brasil...eu falo português\". Como é uma frase comprida que ocupa varias linhas já é necessário um contexto e só é possivel fazer este tipo de previsão utilizando redes neurais recorrentes LSMT.      \n",
    "\n",
    "\n",
    "Nas redes neurais recorrentes simples tem-se a seguinte arquitectura, para um neurónio da camada oculta:  \n",
    "Onde a função de activação por defeito dos neurónios dos loops é a tangente hiperbolica.    \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn5.png\" alt=\"Drawing\" style=\"width: 400px;height: 200px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "Nas redes neurais recorrentes LSTM tem-se a seguinte arquitectura, para um neurónio da camada oculta:  \n",
    "Nas redes neurais recorrentes simples o neurónio recebe a informação, aplica a função de activação e passa a informação para a frente e para ele mesmo.  \n",
    "Nas neurais recorrentes LSTM, antes de passar a informação, para a frente e para ele mesmo, faz-se uma serie de processamentos, como decidir o que será apagado da memoria(Forget gate),o que será armazenado na memoria(Input gate:),etc para decidir o que ler da memória e passar a informação frente e para ele mesmo(Output gate).  \n",
    "As operações de de apagar e adicionar na memória são o que evita os problemas(desaparecimento e explusão do gradiente) no gradiente.      \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn6.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "#### Decidir o que será apagado\n",
    "Utiliza-se a função sigmoide porque retornará valores entre 0 e 1. \n",
    "Se o valor for 0 o dado não é importante e é apagado da memória.    \n",
    "Recebe a saida do tempo anterior(ht-1) e o valor actual(xt) e Utiliza a função sigmoide para decidir o que será apagado.    \n",
    "Por exemplo a memória pode incluir o gênero da pessoa para que os pronomes corretos possam ser usados(Descobrir o género da pessoa para decidir se escre ele ou ela). Quando a rede encontra uma nova pessoa, pode apagar o gênero da pessoa anterior(Se encontra o nome Ana memoria adiciona o pronome Ela, se encontra o nome Bruno adiciona o pronome ele).      \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn7.png\" alt=\"Drawing\" style=\"width: 400px;height: 250px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "#### Decidir o que será armazenado\n",
    "Decidir quais valores serão alterados e a função tangente hiperbólica cria um vetor de novos candidatos.    \n",
    "Por exemplo adicionar o novo gênero da pessoa que foi apagado anteriormente(Se encontra uma pessoa do sexo masculino e estava adicionado uma pessoa do sexo feminino vai ser adicionado o novo género no lugar do que foi apagado anteriormente).   \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn8.png\" alt=\"Drawing\" style=\"width: 400px;height: 250px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "#### Atualizar o estado\n",
    "As etapas anteriores decidiram o que apagar e o que armazenar, e agora essas etapas são executadas.  \n",
    "Como essa informação é adicionada ou removida através do \"canal\" que flui através do tempo, os neurónios seguintes do loop têm acesso a esses dados(no caso de ter sido adicionada à memória).              \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn9.png\" alt=\"Drawing\" style=\"width: 400px;height: 250px;\"/> </td>        \n",
    "</tr></table> \n",
    "\n",
    "#### Decidir qual será a saída para a frente e para o mesmo neurónio\n",
    "Aplica a função sigmoide e coloca os valores entre -1 e 1 para retornar somente as partes necessárias.   \n",
    "Por exemplo se encontrou uma pessoa, pode retornar se é ele ou ela.      \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/rnn10.png\" alt=\"Drawing\" style=\"width: 400px;height: 250px;\"/> </td>        \n",
    "</tr></table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Não Supervisionados <a name=\"naosupervisionados\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapas Auto-Organizáveis (SOM)<a name=\"mapas\"></a>\n",
    "Os mapas auto-organizáveis(Self-Organized Maps) é um algoritmo utilizado para agrupamento(como o k-means) e detecção de outliers que utiliza redes neurais artificiais. Tambem pode ser utilizado para redução de dimensionalidade(seleccionar os atributos principais).      \n",
    "\n",
    "Pode ser utilizado por exemplo para detectar suspeitas de fraudes(outliers). Agrupar os paises por nivel de riqueza(figura seguinte).       \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/mapa.png\" alt=\"Drawing\" style=\"width: 400px;height: 250px;\"/> </td> \n",
    "<td> <img src=\"fotos/mapa1.png\" alt=\"Drawing\" style=\"width: 400px;height: 250px;\"/> </td>     \n",
    "</tr></table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sua estrutura é semelhante às redes neurais tradicionais com apenas camada de entrada e camada de saida.      \n",
    "Tem uma camada de entrada(com os atributos de entrada) directamente ligada a camada de saida.   \n",
    "Cada neurónio da camada de entrada está directamente ligado a todos os neuronios da camada de saida(rede neural densa). A camada  de saida é uma matriz.  \n",
    "A sua representação é feita da forma da figura seginte da direita. Normalmente a camada de saida é representada na forma de matriz, mas tembem pode ser representada na forma tradicional das redes neurais.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/mapa2.png\" alt=\"Drawing\" style=\"width: 400px;height: 300px;\"/> </td> \n",
    "<td> <img src=\"fotos/mapa3.png\" alt=\"Drawing\" style=\"width: 350px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "A diferença para as redes neurais tradicionais reside no facto de não haverem camadas ocultas nem se utilizar funções de\n",
    "activação, nem se multiplicam os valores de entrada pelos pesos.    \n",
    "Nos mapas auto organizáveis os pesos representam o valor do atributo da camada de entrada.    \n",
    "por exemplo:     \n",
    "1º registro: idade:21, salario: 1800      \n",
    "2 registro: idadde:20, salario: 1500      \n",
    "Os pesos apenas representam nós. Por exemplo numa camada de entrada com 2 atributos. No nó 1(neuronio 1) da camada de saida\n",
    "tem-se o valor do peso 1, relacionado com o 1º atributo do 1º registro e o valor do peso 2, relacionado com o 2º atributo do\n",
    "1º registro. Nó 1: (P1,1:P1,2).        \n",
    "No nó 2(neuronio 2) da camada de saida tem-se o valor do peso 1, relacionado com o 1º atributo do 2º registro e o valor do \n",
    "peso 2, relacionado com o 2º atributo do 2º registro. Nó 2: (P2,1:P2,2).   \n",
    "\n",
    "#### Tamanho do mapa(camada de saida)\n",
    "Calculo do tamanho da matriz de saida (SOM-self organizing map):5xsqr(N), sendo N o nº de registros.\n",
    "Por exemplo para 178 registros: 5xsqr(178) = 65,65 celulas = matriz 8x8.  \n",
    "\n",
    "#### Treino do algoritmo:  \n",
    "##### 1º passo\n",
    "Os nós sao inicializados aleatoriamente: pr ex. Nó 1: (30,3000) e Nó 2: (15,1000).  \n",
    "Calcula-se a distancia eucladiana entre cada registro da base de dados e cada nó inicializado aleatoriamente(como no k-means\n",
    "com os registros da base de dados a funcionar de centroides).  \n",
    "Cada registro tem 16 nós inicializados aleatóriamente.  \n",
    "  \n",
    "Para o 1º registro e nó 1: sqr(21-30)^2+(1800-3000)^2=0.22    \n",
    "Para o 1º registro e nó 2: sqr(21-15)^2+(1800-1000)^2=0.10   \n",
    "...  \n",
    "Para o 1º registro e nó 16: sqr(21-..)^2+(1800-..)^2=0.73  \n",
    "  \n",
    "...  \n",
    "...\n",
    "  \n",
    "Para o 2º registro e nó 1: sqr(20-30)^2+(1500-3000)^2    \n",
    "Para o 2º registro e nó 2: sqr(20-15)^2+(1500-1000)^2    \n",
    "...   \n",
    "Para o 2º registro e nó 16: sqr(20-...)^2+(1500-...)^2    \n",
    "...  \n",
    "...  \n",
    "\n",
    "Na figura abaixo estão os calculos para o 1º registro. Tem-se de fazer os mesmos calculos para todos os 6 registros. \n",
    "Na tabela pode-se ver que o BMU para o 1º e 2º registro o BMU foi o Nó 2(cor vermelha), para o 3º e 6 registro foi o Nó 13(cor verde) e para o 4º e 5º registro foi o Nó 9(vermelho escuro).  \n",
    "Os registros que têm os mesmos Nós como BMU são registros são semelhantes, por exemplo (21,1800) e (20,1500).    \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/mapa4.png\" alt=\"Drawing\" style=\"width: 700px;height: 500px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "##### 2º passo\n",
    "Selecciona-se o nó(ponto) que está mais proximo(mais semelhante) de cada registro e atribui-se o nome de BMU-best match unit.\n",
    "Para o 1º registro seria o nó 2(distancia de 0.10). Tem-se de fazer os mesmos calculos para todos os 6 registros.      \n",
    "\n",
    "##### 3º passo\n",
    "De seguida para os nós que não forem escolhidos para BMU cacula-se a distancia eucladiana entre esse nó e os BMU existentes. \n",
    "Considera-se que esse nó pertence ao agrupamento do BMU que estiver mais proximo.    \n",
    "\n",
    "Um registro da base de dados pode estar associado a somente um BMU, mas um BMU pode estar associado a vários registros.  \n",
    "\n",
    "Na figura seguinte tem-se uma matriz 12x8 com 96 nós.  \n",
    "16 nós por registro x 6 registros= 96 nós.  \n",
    "Tem-se os 3 nós de BMU e o nó (?) que não foi considerado BMU.  \n",
    "Como a sua distancia eucladiana é menor para o BMU vermelho esse nó pertencerá a esse agrupamento.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/mapa5.png\" alt=\"Drawing\" style=\"width: 400px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "##### 4º passo  \n",
    "A aprendizagem do modelo é feita aproximando o valor dos BMU dos registros de entrada(inicialmente os valores dos BMU foram\n",
    "obtidos através da inicialização aleatoria dos pesos).  \n",
    "Ao se aproximar os valores dos BMU aos registros de entrada , tambem se está a aproximar os nós restantes (que não foram \n",
    "escolhidos para BMU) aos seus valores de entrada.  \n",
    "\n",
    "Esta aproximação entre o BMU e o registro é feita obtendo a diferença entre os dois valores e incrementando uma parcela desta\n",
    "diferença no BMU ponderada por um fator chamado de taxa de aprendizagem que tem como objectivo suavizar o processo de ajuste \n",
    "dos neurônios nos dados. Obtem-se assim o novo valor do nó para os BMU.  \n",
    "\n",
    "n,novo = n+a*(d-n)    \n",
    "Onde:     \n",
    "n,novo: é o novo valor neurónio do BMU  \n",
    "n: é o valor do neuronio do BMU  \n",
    "a: taxa de aprendizagem  \n",
    "d: registro  \n",
    "n: neuronio BMU  \n",
    "\n",
    "Se a=0.5, n,novo = (n+d)/2. A nova posição de n é a media da sua antiga posição com a posição do registro.  \n",
    "Se a=1, n,novo=d. A nova posição de n é a mesma possição do registro.  \n",
    "\n",
    "##### 5º passo\n",
    "De seguida também os pontos vizinhos são aproximados do BMU. Na atualização dos vizinhos será considerado mais um fator de \n",
    "ponderação descrito por uma função chamada função de vizinhança, que produzirá valores menores quando o BMU e o neuronio \n",
    "vizinho forem mais distantes no espaço matricial (látice). Obtem-se o novo valor do nó para os pontos vizinhos dos BMU.  \n",
    " \n",
    "n,novo = nv,novo = nv+a*(nBMU, nv)a*(d-nv)  \n",
    "Onde:   \n",
    "nv,novo: é o novo valor do neurónio de vizinhança  \n",
    "nv: é o valor do neuronio de vizinhança  \n",
    "a*(nBMU, nv): função de vizinhança  \n",
    "a: taxa de aprendizagem  \n",
    "d: registro  \n",
    "n: neuronio de vizinhança  \n",
    "\n",
    "Na maioria dos casos ainda existe mais um parâmetro destas funções que é o raio que aumenta ou diminui os valores da função\n",
    "de vizinhança.    \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/mapa6.png\" alt=\"Drawing\" style=\"width: 400px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "De época para época o valor do raio vai diminuindo.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/mapa7.png\" alt=\"Drawing\" style=\"width: 700px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "##### 6º passo\n",
    "De seguida volta-se a repetir os passos iterativamente começando por calcular a distancia eucladiana entre cada registro da \n",
    "base de dados e cada nó(com os valores dos nós obtidos na iteração anterior).  \n",
    "No número de epocas define-se quantas vezes se vai fazer o ajuste dos pesos. Sendo o objectivo do processo de treino fazer o mapeamento dos BMUs para os valores originais dos registros, para se conseguir fazer os agrupamentos.    \n",
    "\n",
    "Inicializar os pesos, criar os nós para cada célula/neurônio, calcular a distância de cada registro para nó e escolher o \n",
    "neurônio BMU. De seguida trazer os neurônios para mais perto da camada de entrada.  \n",
    "\n",
    "No fim tem-se um mapa auto-organizável semelhante ao da figura abaixo, com os registros agrupados em 3 grupos(porque se tem apenas 3 BMUs).   \n",
    "Esses grupos podem ser pessoas mais novas com salários mais baixos, pessoas mais velhas com salários mais altos e pessoas mais novas com salários mais altos.      \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/mapa8.png\" alt=\"Drawing\" style=\"width: 300px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boltzmann machines<a name=\"boltzmann\"></a>\n",
    "A Boltzmann machine é um algoritmo utilizado para sistemas de recomendação e para redução de dimensionalidade.  \n",
    "São tambem utilizadas para descrever o estado do sistema, ajustando os pesos do sistema. Utilizada depois de ser feito o treino para monitorizar o estado do sistema(detectar outliers, fazer previsões).   \n",
    "A Boltzmann Machine aprende o comportamento padrão e consegue verificar se existe algum registro afastado desse padrão.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas redes neurais tradicionais tem-se uma camada de entrada de seguida camadas ocultas e por ultimo uma camada de saida e são \n",
    "alimentadas através de feed-forward e calculam o erro através de back-propagation.\n",
    "\n",
    "### Boltzmann Machines\n",
    "Nas Boltzmann Machines todos os neurónios estão interligados e a camada de entrada tambem se actualiza(nas redes neurais\n",
    "tradicionais esses valores mantêm-se pois são a representação da base de dados).  \n",
    "As Boltzmann Machines não têm camada de saida. Apenas se tem camada de entrada e os neurónios da camada oculta.  \n",
    "Os nós de entrada tambem geram dados(um nó podem fazer alterações nos outros ou seja os valores da camada de entrada podem ser\n",
    "alterados pelos outros nós ocultos ou por outro nó da camada de entrada).\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm.png\" alt=\"Drawing\" style=\"width: 350px;height: 250px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "### Restricted Boltzmann Machines\n",
    "Nas Boltzmann Machines conforme o número de nós aumenta o número de conexões tambem aumenta exponencialmente, porque todos\n",
    "os nós estão interligados entre si(ligação todos com todos) o que aumenta muito o poder computacional necessário.  \n",
    "Por isso utiliza-se a Restricted Boltzmann Machines onde as ligações são iguais às redes neurais classicas.   \n",
    "Apenas têm a ligação da camada de entrada à camada oculta e não possuem ligações entre os neurónios da camada de entrada nem ligações entre os neurónios da mesma camada ocuta.   \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm1.png\" alt=\"Drawing\" style=\"width: 300px;height: 150px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "\n",
    "#### RBM para sistemas de recomendação:\n",
    "Nos registros tem-se 1 se a pessoa gostou do filme, 0 se não gostou e nulo(_) se não viu o filme.   \n",
    "Por exemplo o primeiro registro (1_10_0).  \n",
    "Na camada de entrada definem-se os nomes dos filmes e na camada oculta divide-se os filmes pelo seu tipo:\n",
    "Numa camada de entrada com 6 neurónios: A bruxa, Invocação do mal, chamado, Se beber não case, Gente grande, American Pie.\n",
    "Numa camada oculta com 2 neurónios: A BM aprende que um neurónio indica filmes de terror e outro neurónio indica filmes de\n",
    "comédia(podia ser actor, realizador).   \n",
    "Não é possivel definir qual neurónio indica o tipo de filme porque o algoritmo funciona como uma caixa preta. O sistema através da aprendizagem da base de dados e com a criação de padrões define um neurónio para representar cada tipo de filme.  \n",
    "Aprende a definir os nós da camada escondida de acordo com as caracteristicas(cada nó é um padrão).  \n",
    "\n",
    "Através da interconectividade das notas dos utilizadores define os padrões(pessoas com gostos semelhantes provavelmente vão\n",
    "ver o mesmos filmes). Porque provavelmente existe alguma caracteristica que os filmes possuem que faz com que as pessoas \n",
    "gostem deles. Se a pessoa gosta de filmes de terror o neurónio da camada escondida definido como terror é activado.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm2.png\" alt=\"Drawing\" style=\"width: 700px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "### Processo de aprendizagem(Algoritmo de aprendizagem: Constractive divergence)\n",
    "\n",
    "#### Constractive divergence\n",
    "Constractive divergence: Alimentar a camada de entrada com dados de treino e executar o amostrador de Gibbs apenas uma vez(Essa etapa é chamada de reconstrução).   \n",
    "Obter uma estimativa dos neurónios da camada de entrada e da camada oculta para modelos com muitos neurónios na camada de \n",
    "entrada pode ser pode ser um processo demorado se for feito por por meio do amostrador de Gibbs utilizando dados aleatorios \n",
    "a alimentar a camada de entrada.     \n",
    "Por isso utiliza-se o Constractive divergence que é mais rápido(executar apenas uma amostragem de Gibbs em vez de executar o amostrador de gibbs por muitas iterações para conseguir a aproximação desejada).  \n",
    "Executa-se apenas uma vez o amsotrador de gibbs porque é suficiente para se conseguir uma aproximação boa o suficiente para o treino.    \n",
    "\n",
    "A impossibilidade de uma solução analitica para o processo de treino, faz com que as RBM tenham que ser treinadas com  um método de gradiente, assim como outras redes neurais. A diferença é que nas RBM o gradiente obtido é uma aproximação, já que o calculo exacto dele leva a um número exponencial de operações.   \n",
    "Esse calculo da aproximação do gradiente foi chamado de Divergencia contrastiva ou Contrastive divergence. Consegue-se obter essa aproximação usando a amostragem de Gibbs.    \n",
    "Nas RBM a aproximaçao do gradiente equivale à aproximação entre o valor do exemplo original e o valor obtido na sua reconstrução.        \n",
    "\n",
    "#### Processo de treino\n",
    "##### A 1ª parte do treino é o gibbs sampling(obter o vector final)\n",
    "Multiplicar os valores da camada de entrada pelos pesos(definidos aleatoriamente) para se obter os valores da camada oculta.\n",
    "Com os valores da camada oculta reconstruir os valores da camada de entrada(registros).  \n",
    "Com os novos valores da camada de entrada obter os valores da camada oculta(multiplicando esses valores pelos pesos).\n",
    "Com os novos valores da camada oculta reconstruir os valores da camada de entrada.\n",
    "Estes passos são repetidos até um determinado numero de épocas ou até os registros obtidos na ultima iteração serem iguais \n",
    "aos registros originais.  \n",
    "\n",
    "Esta reconstrução dos nós é o gibbs sampling. \n",
    "Onde com os valores da camada de entrada e usando a seguinte equação da esquerda se prevê os valores da camada oculta. \n",
    "E com os valores da camada oculta e usando a equação da direita calculam-se os novos valores da camada de entrada v.  \n",
    "\n",
    "Cada neurónio da RBM apenas existe num estádo binário de 0(desactivo) ou 1(activo).  \n",
    "A equação da direita para um vector de entrada v a probabilidade de um neurónio j da camada oculta ser activado pode ser descrida por:    \n",
    "E do mesmo modo, a equação da esquerda descreve, a probabilidade do estado binário de um neurónio j da camada de entrada estar definido como 1(activado) é:     \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm10.png\" alt=\"Drawing\" style=\"width: 800px;height: 100px;\"/> </td>  \n",
    "<td> <img src=\"fotos/bm11.png\" alt=\"Drawing\" style=\"width: 800px;height: 100px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "Após k iterações(normalmente uma iteração é suficiente) obtem-se outro vector de entrada v_k que foi recreado do vector original v_0.    \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm12.png\" alt=\"Drawing\" style=\"width: 800px;height: 100px;\"/> </td>      \n",
    "</tr></table> \n",
    "\n",
    "Cada nó da camada de entrada é reconstruido utilizando todos os nós da camada oculta.\n",
    "Os pesos não são actualizados durante os calculos. Os pesos aleatorios definidos no inicio mantêm-se durante todo o processo. \n",
    "Só são actualizados no fim do processo(depois de todas as épocas ou depois dos registros obtidos na ultima epoca serem iguais\n",
    "aos registros originais).\n",
    "\n",
    "O que o algoritmo está a tentar fazer é através dos valores da camada de entrada(com por exemplo 6 neurónios) gerar outros\n",
    "novos valores(por exemplo 2 neurónios) e com esses novos valores(por exemplo 2 neurónios) gerar os valores da camada de\n",
    "entrada original(com por exemplo 6 neurónios).  \n",
    "Ou seja funciona como a redução da dimensionalidade(quando a camada de saida tem menos neuronios do que a camada de entrada). Por exemplo tenta-se passar de 6 dimensões para 2 dimensões mantendo o máximo de informação possivel.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm3.png\" alt=\"Drawing\" style=\"width: 800px;height: 100px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "##### A 2ª parte do treino é o Contrastive Divergence(actualização dos pesos)\n",
    "Os vectores v_0 e v_k são usados para calcular a probabibildade dos neurónios das camadas ocultas h_0 e h_k estarem activos(equação acima da esquerda).   \n",
    "Resultando a seguinte matriz de actualização dos pesos:  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm13.png\" alt=\"Drawing\" style=\"width: 250px;height: 50px;\"/> </td>     \n",
    "</tr></table> \n",
    "Sendo p(h0|v0) a probabilidade de um neurónio da camada h0 estar activo dado o vector v0 e p(hk|vk) a probabilidade de um neurónio da camada hk estar activo dado o vector vk.    \n",
    "\n",
    "A actualização dos pesos é feita utilizando um método do gradiente. \n",
    "Na equação seguinte seguida mostra-se a actualização dos pesos utilizando o gradiente ascendente(poderia-se utilizar o gradiente descendente).  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm14.png\" alt=\"Drawing\" style=\"width: 250px;height: 50px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "\n",
    "### Processo de recomendação\n",
    "Partindo do principio que durante o treino do algoritmo encontrou-se 1 nó para filmes de terror e outro nó para filmes de \n",
    "comédia.\n",
    "Para 1 registro com os seguintes dados: 1_100_ (ou seja gostou de 2 filmes de terror e não gostou de 2 filmes de comédia).\n",
    "Não viu um dos filmes de terror e 1 dos filmes de comédia.\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm4.png\" alt=\"Drawing\" style=\"width: 300px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n",
    "Compara-se os neuronios da camada de entrada, com os filmes que a pessoa viu, com os neuronios da camada oculta. \n",
    "Ligam-se os filmes A bruxa e o chamado(filmes de terror vistos) ao neurónio dos filmes de terror. Como a pessoa gostou desses\n",
    "2 filmes esse neurónio é activado(fica verde).\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm5.png\" alt=\"Drawing\" style=\"width: 300px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "Ligam-se os filmes Se beber não case, Gente grande(filmes de comédia que a pessoa viu) ao neurónio dos filmes de comédia. \n",
    "Como a pessoa não gostou desses filmes esse neurónio é desactivado(fica vermelho).\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm6.png\" alt=\"Drawing\" style=\"width: 300px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "Para o filmes não vistos, identifica-se se pertence a terror ou comédia e se os neurónios da camada oculta estão actidos ou\n",
    "desactivados.  \n",
    "Para o Invocação do mal, como é um filme de terror vai estar ligado apenas ao neurónio de terror. Como o neurónio de terror \n",
    "está activo(verde) vai retornar o valor 1 para a camada de entrada(ou seja o filme vai ser recomendado).\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm7.png\" alt=\"Drawing\" style=\"width: 300px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n",
    "Para o American Pie, como é um filme de comédia vai estar ligado apenas ao neurónio de comédia. Como o neurónio de comédia\n",
    "está desactivo(vermelho) vai retornar o valor 0 para a camada de entrada(ou seja o filme não vai ser recomendado).\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/bm8.png\" alt=\"Drawing\" style=\"width: 300px;height: 200px;\"/> </td>     \n",
    "</tr></table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders <a name=\"autoencoders\"></a>\n",
    "\n",
    "O algoritmo autoencoder é usado para redução da dimensionalidade.   \n",
    "É um algoritmo self-supervised learning, aprendizagem por si próprio. O que o algoritmo faz é aprender a classificar os\n",
    "registros da camada de saida de acordo com os valores da camada de entrada. O algoritmo aprende a codificar e descodificar\n",
    "por si próprio. É como se os registros codificados fossem uma classe.  \n",
    "\n",
    "O funcionamento do algoritmo restricted boltzman machine, com o treino feito utilizando o contrastive divergence com uma iteração da amostragem de gibbs é semelhante a um autoencoder. Com a camada de codificação a ser equivalente a camada oculta da RBM.     \n",
    "\n",
    "Tem uma camada de entrada, uma camada oculta e a camada de saida.      \n",
    "O nº de neurónios da camada de saida é igual ao da camada de entrada.  \n",
    "O nº de neurónios da camada oculta é inferior ao da camada de entrada, de modo a fazer a redução de dimensionalidade.    \n",
    "Cada neuronio da camada de entrada está ligado a todos os neurónios da camada oculta(fully connected).    \n",
    "Cada neurónio da camada oculta está ligado a todos os neurónios da camada de saida(fully connected).  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/e.png\" alt=\"Drawing\" style=\"width: 400px;height: 400px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "Da camada de entrada para a camada oculta faz-se uma codificação dos valores da camada de entrada utilizando os pesos.  \n",
    "Por ex. tem-se na camada de entrada (1 0 1 0 1) faz-se uma codificação e tem-se na camada oculta esses valores codificados\n",
    "por exemplo (2 3). Da camada oculta para a camada de saida faz-se a descodificação, utilizando os pesos, e tem-se na camada \n",
    "de saida o valor original da camada de entrada (1 0 1 0 1).  \n",
    "\n",
    "O processo é parecido às redes neurais classicas feed-forward. Utiliza-se tambem o back-propagation para a actualização dos \n",
    "pesos.  \n",
    "São atribuidos pesos aos neuronios da rede e os calculos são efectuados com esses pesos partindo da camada de entrada até a \n",
    "camada de saida.   \n",
    "Calcula-se o erro obtido na camada de saida e modificam-se os pesos da iteração anterior partindo da camada de saida até a \n",
    "camada de entrada. Utiliza-se o método da descida do gradiente para encontrar o melhor valor para os pesos.  \n",
    "De seguida, e com os pesos obtidos na iteração anterior, são efectuados os mesmos calculos. Cada actualização dos pesos\n",
    "chama-se de epoca.  \n",
    "\n",
    "### Exemplo\n",
    "Para 4 variaveis de entrada emprego, filhos, casado, carro, tem-se o seguinte registro 0010.\n",
    "\n",
    "Codificação (Representação com 2 dimensões da camada de entrada):  \n",
    "A ligação da camada de entrada a camada oculta faz-se atraves de linhas(representando os pesos).   \n",
    "Para exemplificação representou-se a linha sólida por +1 e a linha a tracejado por -1.   \n",
    "Na prática estes valores são inicializados aleatoriamente, como os pesos e o objetivo é encontrar o melhor conjunto de pesos assim como nas redes neurais tradicionais.  \n",
    "\n",
    "A ligação do neurónio emprego ao neuronio 1 da camada oculta faz-se através de uma linha sólida.  \n",
    "A ligação do neurónio casado ao neuronio 1 da camada oculta faz-se através de uma linha sólida.  \n",
    "A ligação do neurónio casado ao neuronio 2 da camada oculta faz-se através de uma linha sólida.  \n",
    "A ligação do neurónio carro ao neuronio 2 da camada oculta faz-se através de uma linha sólida.  \n",
    "Todas as restantes ligações fazem-se através de linhas tracejadas.  \n",
    "Para o neurónio 1 da camada oculta tem-se: (0x1)+(0x-1)+(1x1)+(0x-1)=1  \n",
    "Para o neurónio 2 da camada oculta tem-se; (0x-1)+(0x-1)+(1x1)+(0x1)=1  \n",
    "\n",
    "Descodificação:  \n",
    "A ligação da camada de oculta faz-se atraves de linhas(representando os pesos). Para exemplificação\n",
    "representou-se a linha sólida por +1 e a linha a tracejado por -1. Na prática estes valores são inicializados aleatoriamente,\n",
    "como os pesos.  \n",
    "A ligação do neurónio 1 ao neurónio 1 da camada de saida (emprego) faz-se através de 1 linha sólida.  \n",
    "A ligação do neurónio 1 ao neurónio 3 da camada de saida (casado) faz-se através de 1 linha sólida.  \n",
    "A ligação do neurónio 2 ao neurónio 2 da camada de saida (filhos) faz-se através de 1 linha sólida.  \n",
    "A ligação do neurónio 2 ao neurónio 3 da camada de saida (casado) faz-se através de 1 linha sólida.  \n",
    "Para o neurónio 1 da camada de saida(emprego) tem-se: (1x1)+(1x-1) = 0  \n",
    "Para o neurónio 2 da camada de saida(filhos) tem-se: (1x-1)+(1x1) = 0  \n",
    "Para o neurónio 3 da camada de saida(casado) tem-se: (1x1)+(1x1) = 2  \n",
    "Para o neurónio 4 da camada de saida(carro) tem-se: (1x-1)+(1x-1) = -2  \n",
    "Aplicação de uma função de activação, que pode ser a função softmax (vai transformar todos os valores menores ou iguais a zero\n",
    "em zero e os valores maiores a zero em um).    \n",
    "Para o neurónio 1 da camada de saida(emprego) tem-se: (1x1)+(1x-1) = 0 -> 0  \n",
    "Para o neurónio 2 da camada de saida(filhos) tem-se: (1x-1)+(1x1) = 0 -> 0  \n",
    "Para o neurónio 3 da camada de saida(casado) tem-se: (1x1)+(1x1) = 2 -> 1  \n",
    "Para o neurónio 4 da camada de saida(carro) tem-se: (1x-1)+(1x-1) = -2 -> 0  \n",
    "\n",
    "Neste caso na primeira iteração já se obteve os mesmos valores na camada de saida e de entrada. O que significa que o registro\n",
    "de 4 dimensões 0010 pode ser representado por um registro de 2 dimensões 11, que foi o valor obtido na camada oculta.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/e1.png\" alt=\"Drawing\" style=\"width: 700px;height: 400px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "### Outro tipo de Autoencoders  \n",
    "Em vez de utilizar o autoencoder para reduzir a dimensionalidade, o autoencoder pode ser utilizado para aumentar a\n",
    "dimensionalidade. Isto pode ser feito colocando mais neuronios na camada oculta do que na camada de entrada. Pode ser feito\n",
    "para se ter mais atributos na base de dados, ter uma representação mais detalhada dos dados.   \n",
    "Um dos problemas que pode acontecer é que durante a codificação os valores da camada de entrada podem ser simplesmente \n",
    "copiados para a camada oculta.  \n",
    "Para solucionar este problema foram criados os seguintes autoencoders.\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/e2.png\" alt=\"Drawing\" style=\"width: 250px;height: 250px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "#### Sparce autoencoder:  \n",
    "Este autoencoder utiliza uma tecnica de regularização para prevenir o overfiting.  \n",
    "Não utiliza todos os neurónios da camada oculta(coloca valores pequenos nos pesos). O sparce autoencoder a cada epoca não \n",
    "considera alguns neuronios aleatorios da camada oculta. Por exemplo durante o treino, na primeira época não considera o \n",
    "neuronio 1 da camada oculta. Na segunda época não considera o neuronio 1 e neuronio 4 da camada oculta.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/e3.png\" alt=\"Drawing\" style=\"width: 200px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "\n",
    "#### Denoising autoencoder:\n",
    "Todas as camadas têm o mesmo número de neurónios.  \n",
    "Modifica os valores da camada de entrada , alterando alguns neurónios aleatoriamente para o valor zero durante o treino para\n",
    "evitar o overfitting. Ou seja a cada epoca não considera alguns neuronios aleatorios da camada de entrada.   \n",
    "Quando os pesos são actualizados, a camada de saida é comparada com os valores originais para se obter o valor do erro.    \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/e4.png\" alt=\"Drawing\" style=\"width: 200px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "#### Contractive autoencoder\n",
    "Adiciona uma função de custo quando os pesos são actualizados. Funciona da mesma forma do que os autoencoders tradicionais, \n",
    "apenas durante o processo de back-propagation para o calculo dos pesos é adicionada uma função custo à função do calculo do\n",
    "erro. Isto faz com que o modelo se adpte melhor aos dados.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/e5.png\" alt=\"Drawing\" style=\"width: 250px;height: 250px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "#### Deep autoencoder(stack autoencoder)\n",
    "Por exemplo se predende-se reduzir a dimensionalidade de 5 para 2.\n",
    "Na codificação:  \n",
    "Tem-se a camada de entrada com 5 neurónios  \n",
    "Tem-se de seguida uma camada oculta com 4 neurónios.  \n",
    "Tem-se de seguida uma camada oculta com 3 neurónios.  \n",
    "Tem-se de seguida uma camada oculta com 2 neurónios. A camada com a redução da dimensionalidade pretendida.  \n",
    "Na descodificação:  \n",
    "Tem-se de seguida uma camada oculta com 3 neurónios.  \n",
    "Tem-se de seguida uma camada oculta com 4 neurónios.  \n",
    "Tem-se a camada de saida com 5 neurónios.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/e6.png\" alt=\"Drawing\" style=\"width: 400px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "#### Convolution autoencoder\n",
    "Por exemplo tem-se uma imagem do seguinte tamanho 28x28.  \n",
    "Na codificação:  \n",
    "Camada de entrada: 28x28x1 (1 porque é uma figura)  \n",
    "Camada de convolução(e pooling): 14x14x32 (matriz de pooling obtida por 32 detectores de caracteristicas).  \n",
    "Camada de convolução(e pooling): 7x7x64  \n",
    "Camada de convolução(e pooling): 3x3x128  \n",
    "Camada de flatter: 1152 (3x3x128)  \n",
    "Camada oculta com 10 neurónios. A camada com a redução da dimensionalidade pretendida (neste caso 10 neurónios).  \n",
    "Na descodificação:  \n",
    "Camada de flatter: 1152 (3x3x128)  \n",
    "Camada de convolução(e pooling): 3x3x128  \n",
    "Camada de convolução(e pooling): 7x7x64  \n",
    "Camada de convolução(e pooling): 14x14x32  \n",
    "Camada de saida: 28x28x1  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/e7.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Adversariais Generativas(GANs) <a name=\"gans\"></a>\n",
    "\n",
    "As redes adversariais generativas são algoritmos utilizados para geração de dados, como por exemplo criação de imagens.    \n",
    "\n",
    "É Rede neural que pode criar dados. Aprende com os dados e cria novos dados(outras versões dos dados aprendidos que não \n",
    "existiam anteriormente).  \n",
    "\n",
    "Pode ser utilizada para:\n",
    "Criar textos. Por exemplo com base nos textos de um poeta pode criar poesia com as caracteristicas desse poeta.  \n",
    "Aumentar a resolução de imagens.  \n",
    "Desenhar automáticamente.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan.png\" alt=\"Drawing\" style=\"width: 400px;height: 200px;\"/> </td>     \n",
    "</tr></table> \n",
    "<table><tr>\n",
    "Converter texto em imagem.\n",
    "<td> <img src=\"fotos/gan1.png\" alt=\"Drawing\" style=\"width: 600px;height: 100px;\"/> </td>     \n",
    "</tr></table>   \n",
    "Converter imagem em imagem. Por exemplo tem-se a visão noturna de uma fachada e cria-se a imagem real da fachada. Passar\n",
    "a imagem de preto e branco para cores. Tem-se as bordas da imagem tracejadas e cria-se a imagem completa.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan3.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table>     \n",
    "Geração de automáticas de imagens. Google Deep Mind.\n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan2.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "Criação de Ambientes. Por exemplo gerar várias opções de design interior para uma divisão da casa.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Aprendizagem de criação de imagens.\n",
    "A GAN possui 2 componentes:  \n",
    "O gerador que cria as imagens e o discriminador que acessa as imagens criadas e informa se elas são parecidas às originais.\n",
    "As 2 componenetes Começam do zero, ou seja não sabem nada sobre as imagens, e aprendem sozinhos.   \n",
    "O gerador aprende a criar imagens e o discriminador aprende a avaliar as imagens. Ou seja tem-se 2 redes neurais.  \n",
    "1 rede neural para aprender a criar imagens e outra rede neural para apender a avaliar as imagens. \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan4.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "O gerador tem como entrada um vector com números aleatorios e gera uma imagem com base nesse vector. Com o avançar das épocas\n",
    "a rede neural vai aprendendo(melhorando os pesos) e vai gerar imagens mais parecidas às imagens pretendidas.  \n",
    "O discriminador recebe como parametro as imagens pretendidas e recebe tambem imagens diferentes das pretendidas. Tem-se um\n",
    "problema de classificação binária(imagens pretendidas e imagens não pretendidas).  \n",
    "Tanto se pode utilizar redes neurais densas como redes neurais convolucionais, que são especificas para imagens. \n",
    "Como se pode ver na proxima figura, as primeiras imagens geradas são muito diferentes das pretendidas(neste caso um cão).  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan5.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "#### 1ª itapa:\n",
    "##### Gerador: \n",
    "Recebe numeros aleatorios e gera imagens. Como a rede começa a aprender do zero estas primeiras imagens são\n",
    "muito diferentes da pretendida. As imagens são constituidas pelo conjunto de pixeis gerado pela rede(A imagem é um conjunto\n",
    "de pixeis que variam entre 0 e 255 ou seja a imagem irá ser uma matriz de números).  \n",
    "\n",
    "##### Discriminador: \n",
    "As primeiras imagens geradas são passadas para o discriminador e tambem a base de dados com as imagens pretendidas é passada para o discriminador. Tem-se um problema de classificação binária(imagens pretendidas e imagens não pretendidas), com 1 neuronio na camada de saida e função de activação sigmoid.  \n",
    "  \n",
    "Nesta época o discriminador ainda não foi treinado e não sabe ainda quais são as imagens pretendidas. Vai retornar a \n",
    "probabilidade das imagens geradas serem as não pretendidas(o esperado é que a saida seja 0). E vais retornar a probabilidade\n",
    "das imagens da base de dados com as imagens pretendidas(o esperado é que a saida seja 1).  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan6.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "Por exemplo se tiver-se 500 imagens de cães(da base de dados) e 500 imagens de não cães(do gerador) a rede neural vai fazer\n",
    "o treino durante várias épocas utilizado o algoritmo de back propagation para calculo do erro e vai actualiar os pesos.   \n",
    "Nesta etapa o discriminador vai aprender o que é cão e o que não é. Depois do treino feito o discriminador já não necessita \n",
    "de ter como entrada a base de dados dos cães porque a rede neural já foi treinada.  \n",
    "Volta-se a passar as imagens do gerador(já sem a base de dados dos cães) para o classficador(já treinado) fazer a classificação dessas imagens(quer-se que a saida aproxime-se de 1). Por exemplo tem-se a seguinte classificação(0.3 0.6 0.3).  \n",
    "Compara-se a classificação com o valor 1 e calcula-se o erro.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan7.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "A classificação obtida no discriminador é enviada para o gerador. Está-se a indicar ao gerador que as imagens ainda não estão\n",
    "proximas das pretendidas(têm erro). Ou seja o gerador passa as imagens geradas para o discriminador e o discriminar classifica\n",
    "essas imagens e diz que o erro ainda é demasiado alto. Indica que é nescessário melhorar as imagens(por exemplo o cão precisa\n",
    "de orelhas, rabo, etc.). Isso é feito pelo calculo do erro.  \n",
    "Depois do discriminador passar o feedback para o gerador o processo é iniciado novamente.   \n",
    "\n",
    "#### 2 itapa:\n",
    "##### Gerador: \n",
    "Recebe numeros aleatorios e gera imagens. Estas imagens já vão ser mais semelhantes às pretendidas.  \n",
    "\n",
    "##### Discriminador: \n",
    "As imagens geradas são passadas para o discriminador e tambem a base de dados com as imagens pretendidas é passada para o discriminador. O discriminador vai retornar a probabilidade das imagens geradas serem as não pretendidas(o esperado é que a saida seja 0). E vai retornar a probabilidade das imagens da base de dados com as imagens pretendidas(o esperado é que a saida seja 1).   \n",
    "A rede neural discriminador vai fazer o treino durante várias épocas utilizado o algoritmo de back propagation para calculo do erro e vai actualiar os pesos. Nesta etapa o discriminador vai aprender o que é cão e o que não é.  \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan8.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "Depois do treino feito o discriminador já não necessita de ter como entrada a base de dados dos cães porque a rede neural já foi treinada.      \n",
    "Volta-se a passar as imagens do gerador(já sem a base de dados dos cães) para o discriminador(já treinado) fazer a \n",
    "classificação dessas imagens(quer-se que a saida aproxime-se de 1).  Ou seja o gerador envia as imagens para o discriminador analisar e indicar(calcular o erro) o que deve ser melhorado nessas imagens.  \n",
    "\n",
    "Por exemplo tem-se a seguinte classificação(0.6 0.7 0.5).  \n",
    "Compara-se a classificação com o valor 1(imagens de cães) e calcula-se o erro.      \n",
    "A classificação obtida no discriminador é enviada para o gerador, para este saber o que melhorar.      \n",
    "<table><tr>\n",
    "<td> <img src=\"fotos/gan9.png\" alt=\"Drawing\" style=\"width: 600px;height: 300px;\"/> </td>     \n",
    "</tr></table> \n",
    "\n",
    "O discriminador vai dar o feedback da classificação para o gerador. A rede neural geradora vai pegar no erro obtido na classificação e fazer o treino da rede e gerar novas imagens de modo a melhorar esse erro.      \n",
    "O feedback pode ser por exemplo que as imagens têm olhos a mais, bocas a mais, etc. Este processo é repetido até se obterem imagens semelhantes às pretendidas.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia <a name=\"bibliografia\"></a>\n",
    "\n",
    "## Tensorflow 2.x vs Tensorflow 1.x\n",
    "- https://hackernoon.com/tensorflow-is-dead-long-live-tensorflow-49d3e975cf04    \n",
    "- https://medium.com/data-hackers/tensorflow-2-0-melhores-pr%C3%A1ticas-e-o-que-mudou-ec56ba95b6a \n",
    "- https://medium.com/coinmonks/8-things-to-do-differently-in-tensorflows-eager-execution-mode-47cf429aa3ad\n",
    "\n",
    "##  Transformação de variaveis categóricas em quantitativas \n",
    "- https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
    "- https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "\n",
    "- https://www.quora.com/Minmaxscaler-vs-Standardscaler-Are-there-any-specific-rules-to-use-one-over-the-other-for-a-particular-application\n",
    "- https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n",
    "- https://jovianlin.io/feature-scaling/  \n",
    "\n",
    "## Redes Neurais Artificiais\n",
    "### Funções de activação\n",
    "- https://matheusfacure.github.io/2017/07/12/activ-func/\n",
    "- http://deeplearningbook.com.br/funcao-de-ativacao/\n",
    "- https://sefiks.com/2017/05/15/step-function-as-a-neural-network-activation-function/\n",
    "\n",
    "\n",
    "- https://stats.stackexchange.com/questions/271701/why-is-step-function-not-used-in-activation-functions-in-machine-learning\n",
    "- https://stats.stackexchange.com/questions/263768/can-a-perceptron-with-sigmoid-activation-function-perform-nonlinear-classificati\n",
    "\n",
    "### Backpropagation\n",
    "- http://www.decom.ufop.br/imobilis/wp-content/uploads/2012/06/03_Feedforward-e-Backpropagation1.pdf\n",
    "- https://www.devmedia.com.br/redes-neurais-artificiais-algoritmo-backpropagation/28559\n",
    "- http://deeplearningbook.com.br/algoritmo-backpropagation-parte-2-treinamento-de-redes-neurais/\n",
    "\n",
    "### Tipos de descida do gradiente\n",
    "- https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
    "- https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/  \n",
    "- https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1  \n",
    "\n",
    "### Calculo da descida do gradiente\n",
    "- https://pt.wikipedia.org/wiki/Gradiente\n",
    "- https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
    "- https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent\n",
    "- https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#sigmoid-activation\n",
    "- https://towardsdatascience.com/visualising-relationships-between-loss-activation-functions-and-gradient-descent-312a3963c9a5\n",
    "  \n",
    "- https://stats.stackexchange.com/questions/401489/what-does-having-a-stronger-gradient-mean-intuitively-when-talking-about-various/401492  \n",
    "- https://datascience.stackexchange.com/questions/30676/role-derivative-of-sigmoid-function-in-neural-networks\n",
    "  \n",
    "### Redes neurais vs SVM\n",
    "- https://www.quora.com/Why-is-SVM-equal-to-a-2-layer-neural-network\n",
    "- https://www.quora.com/What-is-difference-between-SVM-and-Neural-Networks\n",
    "- https://stackoverflow.com/questions/8963937/svm-and-neural-network\n",
    "\n",
    "### Redes neurais multi-camadas\n",
    "- https://www.quora.com/Why-do-neural-networks-need-more-than-one-hidden-layer\n",
    "- https://stackoverflow.com/questions/41410317/why-do-we-have-multiple-layers-and-multiple-nodes-per-layer-in-a-neural-network\n",
    "- https://stackoverflow.com/questions/34723489/how-do-multiple-hidden-layers-in-a-neural-network-improve-its-ability-to-learn\n",
    "- https://www.quora.com/Why-exactly-do-neural-networks-need-multiple-layers-deep-learning-Please-provide-a-real-example\n",
    "\n",
    "### Bias\n",
    "- https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks\n",
    "  \n",
    "## Redes Neurais Convolucionais\n",
    "- https://www.linkedin.com/pulse/o-que-%C3%A9-um-rede-neural-convolucional-alex-fernandes-mansano\n",
    "- https://mlnotebook.github.io/post/CNN1/\n",
    "\n",
    "- https://datascience.stackexchange.com/questions/27421/when-are-weights-updated-in-cnn\n",
    "- https://stats.stackexchange.com/questions/200513/how-to-initialize-the-elements-of-the-filter-matrix\n",
    "- https://stats.stackexchange.com/questions/279366/updating-of-filters-in-cnn\n",
    "- https://stats.stackexchange.com/questions/362988/in-cnn-do-we-have-learn-kernel-values-at-every-convolution-layer\n",
    "- https://stats.stackexchange.com/questions/267807/cnn-kernels-updates-initialization\n",
    "\n",
    "### ImageDataGenerator\n",
    "- https://stackoverflow.com/questions/51748514/does-imagedatagenerator-add-more-images-to-my-dataset\n",
    "\n",
    "## Redes Neurais Recorrentes\n",
    "\n",
    "### Dissipação e Explosão do gradiente\n",
    "- http://deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/\n",
    "- https://pt.qwerty.wiki/wiki/Vanishing_gradient_problem  \n",
    "- https://deeplearningbook.com.br/a-matematica-da-dissipacao-do-gradiente-e-aplicacoes-das-rnns/\n",
    "- https://machinelearningmastery.com/exploding-gradients-in-neural-networks/\n",
    "- https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb  \n",
    "- https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/\n",
    "  \n",
    "- https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/\n",
    "\n",
    "## Mapas Auto-Organizáveis\n",
    "- https://pt.wikipedia.org/wiki/Mapas_de_Kohonen\n",
    "\n",
    "## Restricted Boltzmann Machines\n",
    "- https://matheusfacure.github.io/2017/07/30/RBM/\n",
    "\n",
    "### Gibbs Sampling e Contrastive Divergence\n",
    "- https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15\n",
    "- http://www.din.uem.br/sbpo/sbpo2016/pdf/155985.pdf\n",
    "- https://www.maxwell.vrac.puc-rio.br/30285/30285.PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
